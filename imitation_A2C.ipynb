{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd03627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "from stable_baselines3 import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd92ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Program files (x86)\\Anaconda\\lib\\site-packages\\gym\\envs\\registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment Poppy-v0\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Poppy!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Program files (x86)\\Anaconda\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "gym.envs.register(\n",
    "     id='Poppy-v0',\n",
    "     entry_point='gym.envs.classic_control:PoppyEnv',\n",
    "     max_episode_steps=1500,\n",
    ")\n",
    "\n",
    "# Create the gym environment\n",
    "env = gym.make('Poppy-v0')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 145\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9d1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "reward :  0.7499070078911216\n",
      "current step :  5\n",
      "episode :  0\n",
      "reward :  0.6876943601391604\n",
      "current step :  10\n",
      "episode :  0\n",
      "reward :  0.5896542247917235\n",
      "current step :  15\n",
      "episode :  0\n",
      "reward :  0.4433010353158586\n",
      "current step :  20\n",
      "episode :  0\n",
      "reward :  0.27366803799813094\n",
      "current step :  25\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  30\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  35\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  40\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  45\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  50\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  55\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  60\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  65\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  70\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  75\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  80\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  85\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  90\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  95\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  100\n",
      "episode :  0\n",
      "reward :  0.25235144721224284\n",
      "current step :  105\n",
      "episode :  0\n",
      "reward :  0.32795639438621144\n",
      "current step :  110\n",
      "episode :  0\n",
      "reward :  0.37076650271259615\n",
      "current step :  115\n",
      "episode :  0\n",
      "reward :  0.39587587879571473\n",
      "current step :  120\n",
      "episode :  0\n",
      "reward :  0.3922257984634779\n",
      "current step :  125\n",
      "episode :  0\n",
      "reward :  0.3718277206193568\n",
      "current step :  130\n",
      "episode :  0\n",
      "reward :  0.48871767367427427\n",
      "current step :  135\n",
      "episode :  0\n",
      "reward :  0.44326551690478033\n",
      "current step :  140\n",
      "episode :  0\n",
      "reward :  0.3328019052860493\n",
      "current step :  145\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  150\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  155\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  160\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  165\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  170\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  175\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  180\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  185\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  190\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  195\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  200\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  205\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  210\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  215\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  220\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  225\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  230\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  235\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  240\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  245\n",
      "episode :  0\n",
      "reward :  0\n",
      "current step :  250\n",
      "episode :  0\n",
      "reward :  0.31140478764336316\n",
      "current step :  255\n",
      "episode :  0\n",
      "reward :  0.4141787062462795\n",
      "current step :  260\n",
      "episode :  0\n",
      "reward :  0.48668308657057757\n",
      "current step :  265\n",
      "episode :  0\n",
      "reward :  0.49517523917779743\n",
      "current step :  270\n",
      "episode :  0\n",
      "reward :  0.49015612539096565\n",
      "current step :  275\n",
      "episode :  0\n",
      "reward :  0.4796224953868723\n",
      "current step :  280\n",
      "episode :  0\n",
      "reward :  0.4703441044171974\n",
      "current step :  285\n",
      "episode :  0\n",
      "reward :  0.470687853969553\n",
      "current step :  290\n",
      "episode :  1\n",
      "reward :  0.31734723806033777\n",
      "current step :  5\n",
      "episode :  1\n",
      "reward :  0.3286691824469208\n",
      "current step :  10\n",
      "episode :  1\n",
      "reward :  0.3234924219741165\n",
      "current step :  15\n",
      "episode :  1\n",
      "reward :  0.2869571550310241\n",
      "current step :  20\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  25\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  30\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  35\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  40\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  45\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  50\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  55\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  60\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  65\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  70\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  75\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  80\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  85\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  90\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  95\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  100\n",
      "episode :  1\n",
      "reward :  0.25235144721224284\n",
      "current step :  105\n",
      "episode :  1\n",
      "reward :  0.32795639438621144\n",
      "current step :  110\n",
      "episode :  1\n",
      "reward :  0.37076650271259615\n",
      "current step :  115\n",
      "episode :  1\n",
      "reward :  0.39587587879571473\n",
      "current step :  120\n",
      "episode :  1\n",
      "reward :  0.39178195365827334\n",
      "current step :  125\n",
      "episode :  1\n",
      "reward :  0.3718277206193568\n",
      "current step :  130\n",
      "episode :  1\n",
      "reward :  0.5361878129944947\n",
      "current step :  135\n",
      "episode :  1\n",
      "reward :  0.5265820398165878\n",
      "current step :  140\n",
      "episode :  1\n",
      "reward :  0.4112585695586838\n",
      "current step :  145\n",
      "episode :  1\n",
      "reward :  0.26572928884238745\n",
      "current step :  150\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  155\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  160\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  165\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  170\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  175\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  180\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  185\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  190\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  195\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  200\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  205\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  210\n",
      "episode :  1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "\n",
    "policy_architecture = 'MlpPolicy'\n",
    "learning_rate = 0.01\n",
    "entropy_coef = 0.0\n",
    "gamma = 4\n",
    "n_steps = 5\n",
    "\n",
    "\n",
    "\n",
    "model = A2C(\n",
    "    policy=policy_architecture,\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    policy_kwargs=dict(activation_fn=torch.nn.ReLU),  # Example: change activation function\n",
    "    verbose=1,\n",
    "    ent_coef=entropy_coef,\n",
    "    gamma=gamma,\n",
    "    n_steps=n_steps\n",
    ")\n",
    "\n",
    "\n",
    "total_timesteps = 100\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "info = pd.DataFrame(env.infos)\n",
    "info.to_pickle('info.pkl')\n",
    "model.save(\"a2c_poppy_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4720840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward :  0.31734723806033777\n",
      "current step :  5\n",
      "episode :  1\n",
      "reward :  0.3286691824469208\n",
      "current step :  10\n",
      "episode :  1\n",
      "reward :  0.3234924219741165\n",
      "current step :  15\n",
      "episode :  1\n",
      "reward :  0.2869571550310241\n",
      "current step :  20\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  25\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  30\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  35\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  40\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  45\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  50\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  55\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  60\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  65\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  70\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  75\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  80\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  85\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  90\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  95\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  100\n",
      "episode :  1\n",
      "reward :  0.25235144721224284\n",
      "current step :  105\n",
      "episode :  1\n",
      "reward :  0.32795639438621144\n",
      "current step :  110\n",
      "episode :  1\n",
      "reward :  0.37076650271259615\n",
      "current step :  115\n",
      "episode :  1\n",
      "reward :  0.39587587879571473\n",
      "current step :  120\n",
      "episode :  1\n",
      "reward :  0.39178195365827334\n",
      "current step :  125\n",
      "episode :  1\n",
      "reward :  0.3718277206193568\n",
      "current step :  130\n",
      "episode :  1\n",
      "reward :  0.5384411449600158\n",
      "current step :  135\n",
      "episode :  1\n",
      "reward :  0.544218091826512\n",
      "current step :  140\n",
      "episode :  1\n",
      "reward :  0.4414809088958038\n",
      "current step :  145\n",
      "episode :  1\n",
      "reward :  0.2897080300133702\n",
      "current step :  150\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  155\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  160\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  165\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  170\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  175\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  180\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  185\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  190\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  195\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  200\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  205\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  210\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  215\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  220\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  225\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  230\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  235\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  240\n",
      "episode :  1\n",
      "reward :  0\n",
      "current step :  245\n",
      "episode :  1\n",
      "reward :  0.263737884871108\n",
      "current step :  250\n",
      "episode :  1\n",
      "reward :  0.3839923152875687\n",
      "current step :  255\n",
      "episode :  1\n",
      "reward :  0.5114790363414126\n",
      "current step :  260\n",
      "episode :  1\n",
      "reward :  0.5598229419174007\n",
      "current step :  265\n",
      "episode :  1\n",
      "reward :  0.5398462693513373\n",
      "current step :  270\n",
      "episode :  1\n",
      "reward :  0.4960952431175706\n",
      "current step :  275\n",
      "episode :  1\n",
      "reward :  0.4724320373068235\n",
      "current step :  280\n",
      "episode :  1\n",
      "reward :  0.467965800155366\n",
      "current step :  285\n",
      "episode :  1\n",
      "reward :  0.47305511547683377\n",
      "current step :  290\n",
      "episode :  2\n",
      "reward :  0.31734723806033777\n",
      "current step :  5\n",
      "episode :  2\n",
      "reward :  0.3286691824469208\n",
      "current step :  10\n",
      "episode :  2\n",
      "reward :  0.3234924219741165\n",
      "current step :  15\n",
      "episode :  2\n",
      "reward :  0.2869571550310241\n",
      "current step :  20\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  25\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  30\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  35\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  40\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  45\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  50\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  55\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  60\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  65\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  70\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  75\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  80\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  85\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  90\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  95\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  100\n",
      "episode :  2\n",
      "reward :  0.25235144721224284\n",
      "current step :  105\n",
      "episode :  2\n",
      "reward :  0.32795639438621144\n",
      "current step :  110\n",
      "episode :  2\n",
      "reward :  0.37076650271259615\n",
      "current step :  115\n",
      "episode :  2\n",
      "reward :  0.39587587879571473\n",
      "current step :  120\n",
      "episode :  2\n",
      "reward :  0.39178195365827334\n",
      "current step :  125\n",
      "episode :  2\n",
      "reward :  0.3718277206193568\n",
      "current step :  130\n",
      "episode :  2\n",
      "reward :  0.5382114793008411\n",
      "current step :  135\n",
      "episode :  2\n",
      "reward :  0.5449213586298939\n",
      "current step :  140\n",
      "episode :  2\n",
      "reward :  0.4505044173602277\n",
      "current step :  145\n",
      "episode :  2\n",
      "reward :  0.2837965285820677\n",
      "current step :  150\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  155\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  160\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  165\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  170\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  175\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  180\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  185\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  190\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  195\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  200\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  205\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  210\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  215\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  220\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  225\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  230\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  235\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  240\n",
      "episode :  2\n",
      "reward :  0\n",
      "current step :  245\n",
      "episode :  2\n",
      "reward :  0.2610012588433329\n",
      "current step :  250\n",
      "episode :  2\n",
      "reward :  0.3839923152875687\n",
      "current step :  255\n",
      "episode :  2\n",
      "reward :  0.5105226250168992\n",
      "current step :  260\n",
      "episode :  2\n",
      "reward :  0.56044615230272\n",
      "current step :  265\n",
      "episode :  2\n",
      "reward :  0.5390355606391143\n",
      "current step :  270\n",
      "episode :  2\n",
      "reward :  0.4934343975481575\n",
      "current step :  275\n",
      "episode :  2\n",
      "reward :  0.4784777058608147\n",
      "current step :  280\n",
      "episode :  2\n",
      "reward :  0.47109113662103935\n",
      "current step :  285\n",
      "episode :  2\n",
      "reward :  0.46892036162608697\n",
      "current step :  290\n",
      "episode :  3\n",
      "reward :  0.31734723806033777\n",
      "current step :  5\n",
      "episode :  3\n",
      "reward :  0.3286691824469208\n",
      "current step :  10\n",
      "episode :  3\n",
      "reward :  0.3234924219741165\n",
      "current step :  15\n",
      "episode :  3\n",
      "reward :  0.2869571550310241\n",
      "current step :  20\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  25\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  30\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  35\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  40\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  45\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  50\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  55\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  60\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  65\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  70\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  75\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  80\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  85\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  90\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  95\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  100\n",
      "episode :  3\n",
      "reward :  0.25235144721224284\n",
      "current step :  105\n",
      "episode :  3\n",
      "reward :  0.32795639438621144\n",
      "current step :  110\n",
      "episode :  3\n",
      "reward :  0.37076650271259615\n",
      "current step :  115\n",
      "episode :  3\n",
      "reward :  0.39587587879571473\n",
      "current step :  120\n",
      "episode :  3\n",
      "reward :  0.39178195365827334\n",
      "current step :  125\n",
      "episode :  3\n",
      "reward :  0.3718277206193568\n",
      "current step :  130\n",
      "episode :  3\n",
      "reward :  0.5380409193483608\n",
      "current step :  135\n",
      "episode :  3\n",
      "reward :  0.5382042629510951\n",
      "current step :  140\n",
      "episode :  3\n",
      "reward :  0.443095804366686\n",
      "current step :  145\n",
      "episode :  3\n",
      "reward :  0.28262181459256086\n",
      "current step :  150\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  155\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  160\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  165\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  170\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  175\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  180\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  185\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  190\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  195\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  200\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  205\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  210\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  215\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  220\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  225\n",
      "episode :  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward :  0\n",
      "current step :  230\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  235\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  240\n",
      "episode :  3\n",
      "reward :  0\n",
      "current step :  245\n",
      "episode :  3\n",
      "reward :  0.2664902231481568\n",
      "current step :  250\n",
      "episode :  3\n",
      "reward :  0.39499305102472393\n",
      "current step :  255\n",
      "episode :  3\n",
      "reward :  0.5100403785956347\n",
      "current step :  260\n",
      "episode :  3\n",
      "reward :  0.5603319341132066\n",
      "current step :  265\n",
      "episode :  3\n",
      "reward :  0.5399939675452206\n",
      "current step :  270\n",
      "episode :  3\n",
      "reward :  0.4966480277028749\n",
      "current step :  275\n",
      "episode :  3\n",
      "reward :  0.47876527604265023\n",
      "current step :  280\n",
      "episode :  3\n",
      "reward :  0.46564327861241167\n",
      "current step :  285\n",
      "episode :  3\n",
      "reward :  0.4663750321998834\n",
      "current step :  290\n",
      "episode :  4\n",
      "reward :  0.31734723806033777\n",
      "current step :  5\n",
      "episode :  4\n",
      "reward :  0.3286691824469208\n",
      "current step :  10\n",
      "episode :  4\n",
      "reward :  0.3234924219741165\n",
      "current step :  15\n",
      "episode :  4\n",
      "reward :  0.2869571550310241\n",
      "current step :  20\n",
      "episode :  4\n",
      "reward :  0\n",
      "current step :  25\n",
      "episode :  4\n",
      "reward :  0\n",
      "current step :  30\n",
      "episode :  4\n",
      "reward :  0\n",
      "current step :  35\n",
      "episode :  4\n",
      "reward :  0\n",
      "current step :  40\n",
      "episode :  4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[1;32m----> 8\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\gym\\envs\\classic_control\\poppy_env.py:63\u001b[0m, in \u001b[0;36mPoppyEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoppy\u001b[38;5;241m.\u001b[39ml_arm_chain\u001b[38;5;241m.\u001b[39mmotors):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_elbow_y\u001b[39m\u001b[38;5;124m'\u001b[39m):   \n\u001b[1;32m---> 63\u001b[0m         \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoto_position\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m         m\u001b[38;5;241m.\u001b[39mgoto_position(\u001b[38;5;241m90.0\u001b[39m,\u001b[38;5;241m1\u001b[39m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\pypot\\dynamixel\\motor.py:263\u001b[0m, in \u001b[0;36mDxlMotor.goto_position\u001b[1;34m(self, position, duration, control, wait)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_position \u001b[38;5;241m=\u001b[39m position\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m--> 263\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m control \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    266\u001b[0m     goto_linear \u001b[38;5;241m=\u001b[39m GotoLinear(\u001b[38;5;28mself\u001b[39m, position, duration)\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\pypot\\vrep\\__init__.py:46\u001b[0m, in \u001b[0;36mvrep_time.sleep\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     43\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     45\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_time()\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m<\u001b[39m t:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_time() \u001b[38;5;241m<\u001b[39m t0:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\pypot\\vrep\\__init__.py:33\u001b[0m, in \u001b[0;36mvrep_time.get_time\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m     sys_time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m.5\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trial \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not get current simulation time. Make sure the V-REP simulation is running. And that you have added the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m child script to your scene.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\pypot\\vrep\\__init__.py:29\u001b[0m, in \u001b[0;36mvrep_time.get_time\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_time\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_simulation_current_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m         sys_time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m.5\u001b[39m)\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\pypot\\vrep\\io.py:227\u001b[0m, in \u001b[0;36mVrepIO.get_simulation_current_time\u001b[1;34m(self, timer)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\" Gets the simulation current time. \"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_remote_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimxGetFloatSignal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m VrepIOErrors:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mB:\\Program files (x86)\\Anaconda\\lib\\site-packages\\pypot\\vrep\\io.py:323\u001b[0m, in \u001b[0;36mVrepIO.call_remote_api\u001b[1;34m(self, func_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remote_api\u001b[38;5;241m.\u001b[39msimx_return_novalue_flag \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m err:\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVrepIO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# if any(err) and hard_retry:\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m#     print \"HARD RETRY\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# self.stop_simulation() #nope\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m#         return res\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(err):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vec_env = model.get_env()\n",
    "\n",
    "model = A2C.load(\"a2c_poppy_env\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224804b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b677ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc9f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
