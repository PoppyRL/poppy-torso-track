{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c47c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "\n",
    "import sys\n",
    "sys.path.append('gym-examples')\n",
    "import gym_examples\n",
    "from gym.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238796fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Poppy!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.11081392, -0.16128625,  0.05544874, -0.10220852, -0.17995109,\n",
       "        0.07120024], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('gym_examples/Poppy-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8c2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f61b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "current step :  0\n",
      "reward :  -0.5890905182962286\n",
      "episode :  0\n",
      "current step :  1\n",
      "reward :  -0.5758196257708356\n",
      "episode :  0\n",
      "current step :  2\n",
      "reward :  -0.3295587923162729\n",
      "episode :  0\n",
      "current step :  3\n",
      "reward :  -0.5777637831720358\n",
      "episode :  0\n",
      "current step :  4\n",
      "reward :  -0.4870008175189487\n",
      "episode :  0\n",
      "current step :  5\n",
      "reward :  -0.6666000151883069\n",
      "episode :  0\n",
      "current step :  6\n",
      "reward :  -0.4883412857884012\n",
      "episode :  0\n",
      "current step :  7\n",
      "reward :  -0.5106642084293174\n",
      "episode :  0\n",
      "current step :  8\n",
      "reward :  -0.6081133199657484\n",
      "episode :  0\n",
      "current step :  9\n",
      "reward :  -0.5375054945462165\n",
      "episode :  0\n",
      "current step :  10\n",
      "reward :  -0.5631528776670356\n",
      "episode :  0\n",
      "current step :  11\n",
      "reward :  -0.46519133916414424\n",
      "episode :  0\n",
      "current step :  12\n",
      "reward :  -0.40199152331417154\n",
      "episode :  0\n",
      "current step :  13\n",
      "reward :  -0.424342240920932\n",
      "episode :  0\n",
      "current step :  14\n",
      "reward :  -0.558720082705791\n",
      "episode :  0\n",
      "current step :  15\n",
      "reward :  -0.5255396917177645\n",
      "episode :  0\n",
      "current step :  16\n",
      "reward :  -0.31370200141095134\n",
      "episode :  0\n",
      "current step :  17\n",
      "reward :  -0.36599006897633635\n",
      "episode :  0\n",
      "current step :  18\n",
      "reward :  -0.36737682259292526\n",
      "episode :  0\n",
      "current step :  19\n",
      "reward :  -0.5894189878822774\n",
      "episode :  0\n",
      "current step :  20\n",
      "reward :  -0.3108708980371619\n",
      "episode :  0\n",
      "current step :  21\n",
      "reward :  -0.36077450352860707\n",
      "episode :  0\n",
      "current step :  22\n",
      "reward :  -0.49815879885250436\n",
      "episode :  0\n",
      "current step :  23\n",
      "reward :  -0.5124514155449996\n",
      "episode :  0\n",
      "current step :  24\n",
      "reward :  -0.5707376799095273\n",
      "episode :  0\n",
      "current step :  25\n",
      "reward :  -0.3648717619827617\n",
      "episode :  0\n",
      "current step :  26\n",
      "reward :  -0.4492286131192955\n",
      "episode :  0\n",
      "current step :  27\n",
      "reward :  -0.5304123934398831\n",
      "episode :  0\n",
      "current step :  28\n",
      "reward :  -0.4557324331898869\n",
      "episode :  0\n",
      "current step :  29\n",
      "reward :  -0.37798147877477317\n",
      "episode :  0\n",
      "current step :  30\n",
      "reward :  -0.4337795535353492\n",
      "episode :  0\n",
      "current step :  31\n",
      "reward :  -0.40436360046821207\n",
      "episode :  0\n",
      "current step :  32\n",
      "reward :  -0.5926270324860552\n",
      "episode :  0\n",
      "current step :  33\n",
      "reward :  -0.5838755448303771\n",
      "episode :  0\n",
      "current step :  34\n",
      "reward :  -0.5518188767562225\n",
      "episode :  0\n",
      "current step :  35\n",
      "reward :  -0.5616029972007115\n",
      "episode :  0\n",
      "current step :  36\n",
      "reward :  -0.4966676845338198\n",
      "episode :  0\n",
      "current step :  37\n",
      "reward :  -0.41665677388820727\n",
      "episode :  0\n",
      "current step :  38\n",
      "reward :  -0.40000019738539627\n",
      "episode :  0\n",
      "current step :  39\n",
      "reward :  -0.3951388901953076\n",
      "episode :  0\n",
      "current step :  40\n",
      "reward :  -0.4216472183263827\n",
      "episode :  0\n",
      "current step :  41\n",
      "reward :  -0.3864049579267228\n",
      "episode :  0\n",
      "current step :  42\n",
      "reward :  -0.4418196748147624\n",
      "episode :  0\n",
      "current step :  43\n",
      "reward :  -0.4414027340316527\n",
      "episode :  0\n",
      "current step :  44\n",
      "reward :  -0.5298232449255393\n",
      "episode :  0\n",
      "current step :  45\n",
      "reward :  -0.667105724530149\n",
      "episode :  0\n",
      "current step :  46\n",
      "reward :  -0.5195744129580153\n",
      "episode :  0\n",
      "current step :  47\n",
      "reward :  -0.6126076893513512\n",
      "episode :  0\n",
      "current step :  48\n",
      "reward :  -0.5176706349415655\n",
      "episode :  0\n",
      "current step :  49\n",
      "reward :  -0.4159123298686981\n",
      "episode :  0\n",
      "current step :  50\n",
      "reward :  -0.44212068191498555\n",
      "episode :  0\n",
      "current step :  51\n",
      "reward :  -0.5198346507315761\n",
      "episode :  0\n",
      "current step :  52\n",
      "reward :  -0.403070181527896\n",
      "episode :  0\n",
      "current step :  53\n",
      "reward :  -0.3636293845099668\n",
      "episode :  0\n",
      "current step :  54\n",
      "reward :  -0.35904972374613997\n",
      "episode :  0\n",
      "current step :  55\n",
      "reward :  -0.4956988180718468\n",
      "episode :  0\n",
      "current step :  56\n",
      "reward :  -0.5213656475793463\n",
      "episode :  0\n",
      "current step :  57\n",
      "reward :  -0.44038074787613335\n",
      "episode :  0\n",
      "current step :  58\n",
      "reward :  -0.5207104194274859\n",
      "episode :  0\n",
      "current step :  59\n",
      "reward :  -0.46602409616817964\n",
      "episode :  0\n",
      "current step :  60\n",
      "reward :  -0.44325081880942907\n",
      "episode :  0\n",
      "current step :  61\n",
      "reward :  -0.618413760541908\n",
      "episode :  0\n",
      "current step :  62\n",
      "reward :  -0.570100994383862\n",
      "episode :  0\n",
      "current step :  63\n",
      "reward :  -0.7357331556124772\n",
      "episode :  0\n",
      "current step :  64\n",
      "reward :  -0.3928313642505114\n",
      "episode :  0\n",
      "current step :  65\n",
      "reward :  -0.44313947945753823\n",
      "episode :  0\n",
      "current step :  66\n",
      "reward :  -0.5938940953754911\n",
      "episode :  0\n",
      "current step :  67\n",
      "reward :  -0.6762698784446873\n",
      "episode :  0\n",
      "current step :  68\n",
      "reward :  -0.6314092065260322\n",
      "episode :  0\n",
      "current step :  69\n",
      "reward :  -0.6451208043093157\n",
      "episode :  0\n",
      "current step :  70\n",
      "reward :  -0.605337339082431\n",
      "episode :  0\n",
      "current step :  71\n",
      "reward :  -0.6233878288613218\n",
      "episode :  0\n",
      "current step :  72\n",
      "reward :  -0.5841839227716388\n",
      "episode :  0\n",
      "current step :  73\n",
      "reward :  -0.49832047682847397\n",
      "episode :  0\n",
      "current step :  74\n",
      "reward :  -0.7231208353215521\n",
      "episode :  0\n",
      "current step :  75\n",
      "reward :  -0.6210476051061333\n",
      "episode :  0\n",
      "current step :  76\n",
      "reward :  -0.62367934280613\n",
      "episode :  0\n",
      "current step :  77\n",
      "reward :  -0.6390522790356247\n",
      "episode :  0\n",
      "current step :  78\n",
      "reward :  -0.6094703600656001\n",
      "episode :  0\n",
      "current step :  79\n",
      "reward :  -0.47990929198790594\n",
      "episode :  0\n",
      "current step :  80\n",
      "reward :  -0.3369710885572097\n",
      "episode :  0\n",
      "current step :  81\n",
      "reward :  -0.4408397972268568\n",
      "episode :  0\n",
      "current step :  82\n",
      "reward :  -0.5471463526008676\n",
      "episode :  0\n",
      "current step :  83\n",
      "reward :  -0.5254577091820031\n",
      "episode :  0\n",
      "current step :  84\n",
      "reward :  -0.48749047994043077\n",
      "episode :  0\n",
      "current step :  85\n",
      "reward :  -0.30967143476347614\n",
      "episode :  0\n",
      "current step :  86\n",
      "reward :  -0.27530070347549546\n",
      "episode :  0\n",
      "current step :  87\n",
      "reward :  -0.3727953518747631\n",
      "episode :  0\n",
      "current step :  88\n",
      "reward :  -0.5837773013374891\n",
      "episode :  0\n",
      "current step :  89\n",
      "reward :  -0.40321096229840164\n",
      "episode :  0\n",
      "current step :  90\n",
      "reward :  -0.40433396961348284\n",
      "episode :  0\n",
      "current step :  91\n",
      "reward :  -0.5613273856447951\n",
      "episode :  0\n",
      "current step :  92\n",
      "reward :  -0.5059474390619928\n",
      "episode :  0\n",
      "current step :  93\n",
      "reward :  -0.3486016791501446\n",
      "episode :  0\n",
      "current step :  94\n",
      "reward :  -0.3281458078794374\n",
      "episode :  0\n",
      "current step :  95\n",
      "reward :  -0.5323358972386728\n",
      "episode :  0\n",
      "current step :  96\n",
      "reward :  -0.4484874544506405\n",
      "episode :  0\n",
      "current step :  97\n",
      "reward :  -0.5683202051898063\n",
      "episode :  0\n",
      "current step :  98\n",
      "reward :  -0.40304249195084485\n",
      "episode :  0\n",
      "current step :  99\n",
      "reward :  -0.48532167773989704\n",
      "episode :  0\n",
      "current step :  100\n",
      "reward :  -0.25461761315995685\n",
      "episode :  0\n",
      "current step :  101\n",
      "reward :  -0.27081099964118\n",
      "episode :  0\n",
      "current step :  102\n",
      "reward :  -0.2510480230220745\n",
      "episode :  0\n",
      "current step :  103\n",
      "reward :  -0.2834191716186812\n",
      "episode :  0\n",
      "current step :  104\n",
      "reward :  -0.20146950823160303\n",
      "episode :  0\n",
      "current step :  105\n",
      "reward :  -0.11422291464570827\n",
      "episode :  0\n",
      "current step :  106\n",
      "reward :  -0.15605217461858836\n",
      "episode :  0\n",
      "current step :  107\n",
      "reward :  -0.17279551511185368\n",
      "episode :  0\n",
      "current step :  108\n",
      "reward :  -0.11893952910277089\n",
      "episode :  0\n",
      "current step :  109\n",
      "reward :  -0.1809990890475437\n",
      "episode :  0\n",
      "current step :  110\n",
      "reward :  -0.18671147781748096\n",
      "episode :  0\n",
      "current step :  111\n",
      "reward :  -0.23919117085197808\n",
      "episode :  0\n",
      "current step :  112\n",
      "reward :  -0.2226444137989589\n",
      "episode :  0\n",
      "current step :  113\n",
      "reward :  -0.17697678507363354\n",
      "episode :  0\n",
      "current step :  114\n",
      "reward :  -0.06104953350194998\n",
      "episode :  0\n",
      "current step :  115\n",
      "reward :  -0.13149474510140308\n",
      "episode :  0\n",
      "current step :  116\n",
      "reward :  -0.06757074562997933\n",
      "episode :  0\n",
      "current step :  117\n",
      "reward :  -0.08740818726040289\n",
      "episode :  0\n",
      "current step :  118\n",
      "reward :  -0.14198474126628455\n",
      "episode :  0\n",
      "current step :  119\n",
      "reward :  -0.2727907684873888\n",
      "episode :  0\n",
      "current step :  120\n",
      "reward :  -0.17504095501568398\n",
      "episode :  0\n",
      "current step :  121\n",
      "reward :  -0.21812098255304246\n",
      "episode :  0\n",
      "current step :  122\n",
      "reward :  -0.21303433469718408\n",
      "episode :  0\n",
      "current step :  123\n",
      "reward :  -0.1458673439670454\n",
      "episode :  0\n",
      "current step :  124\n",
      "reward :  -0.22138487730496711\n",
      "episode :  0\n",
      "current step :  125\n",
      "reward :  -0.18159926630261736\n",
      "episode :  0\n",
      "current step :  126\n",
      "reward :  -0.12360644276010346\n",
      "episode :  0\n",
      "current step :  127\n",
      "reward :  -0.18702041609889747\n",
      "episode :  0\n",
      "current step :  128\n",
      "reward :  -0.16963426796021253\n",
      "episode :  0\n",
      "current step :  129\n",
      "reward :  -0.14144848520641112\n",
      "episode :  0\n",
      "current step :  130\n",
      "reward :  -0.14519135391771054\n",
      "episode :  0\n",
      "current step :  131\n",
      "reward :  -0.22308642477768525\n",
      "episode :  0\n",
      "current step :  132\n",
      "reward :  -0.14107481335341068\n",
      "episode :  0\n",
      "current step :  133\n",
      "reward :  -0.17881974766448006\n",
      "episode :  0\n",
      "current step :  134\n",
      "reward :  -0.21796137216136882\n",
      "episode :  0\n",
      "current step :  135\n",
      "reward :  -0.1308897253470088\n",
      "episode :  0\n",
      "current step :  136\n",
      "reward :  -0.2207119287747963\n",
      "episode :  0\n",
      "current step :  137\n",
      "reward :  -0.21256588100500065\n",
      "episode :  0\n",
      "current step :  138\n",
      "reward :  -0.12136597593413685\n",
      "episode :  0\n",
      "current step :  139\n",
      "reward :  -0.17287896883333181\n",
      "episode :  0\n",
      "current step :  140\n",
      "reward :  -0.21853266319692863\n",
      "episode :  0\n",
      "current step :  141\n",
      "reward :  -0.14982538591149996\n",
      "episode :  0\n",
      "current step :  142\n",
      "reward :  -0.23984561518414635\n",
      "episode :  0\n",
      "current step :  143\n",
      "reward :  -0.35559402556388336\n",
      "episode :  0\n",
      "current step :  144\n",
      "reward :  -0.29006946384804727\n",
      "episode :  0\n",
      "current step :  145\n",
      "reward :  -0.32090735585819213\n",
      "episode :  0\n",
      "current step :  146\n",
      "reward :  -0.2973198910121419\n",
      "episode :  0\n",
      "current step :  147\n",
      "reward :  -0.3857241446167199\n",
      "episode :  0\n",
      "current step :  148\n",
      "reward :  -0.37226872622570417\n",
      "episode :  0\n",
      "current step :  149\n",
      "reward :  -0.39103335965110386\n",
      "episode :  0\n",
      "current step :  150\n",
      "reward :  -0.4184318049815881\n",
      "episode :  0\n",
      "current step :  151\n",
      "reward :  -0.44041624273313185\n",
      "episode :  0\n",
      "current step :  152\n",
      "reward :  -0.4473863310967813\n",
      "episode :  0\n",
      "current step :  153\n",
      "reward :  -0.40633284696572675\n",
      "episode :  0\n",
      "current step :  154\n",
      "reward :  -0.49637458192745826\n",
      "episode :  0\n",
      "current step :  155\n",
      "reward :  -0.5006470761097725\n",
      "episode :  0\n",
      "current step :  156\n",
      "reward :  -0.3991416501075713\n",
      "episode :  0\n",
      "current step :  157\n",
      "reward :  -0.5137128800206946\n",
      "episode :  0\n",
      "current step :  158\n",
      "reward :  -0.45435445853545675\n",
      "episode :  0\n",
      "current step :  159\n",
      "reward :  -0.5531727345975701\n",
      "episode :  0\n",
      "current step :  160\n",
      "reward :  -0.5626223044230241\n",
      "episode :  0\n",
      "current step :  161\n",
      "reward :  -0.558429203529316\n",
      "episode :  0\n",
      "current step :  162\n",
      "reward :  -0.5399627133718135\n",
      "episode :  0\n",
      "current step :  163\n",
      "reward :  -0.5968559422943875\n",
      "episode :  0\n",
      "current step :  164\n",
      "reward :  -0.5234358212546387\n",
      "episode :  0\n",
      "current step :  165\n",
      "reward :  -0.5039569320470073\n",
      "episode :  0\n",
      "current step :  166\n",
      "reward :  -0.5529587450437781\n",
      "episode :  0\n",
      "current step :  167\n",
      "reward :  -0.516101777799972\n",
      "episode :  0\n",
      "current step :  168\n",
      "reward :  -0.5159184983742656\n",
      "episode :  0\n",
      "current step :  169\n",
      "reward :  -0.5527356311828066\n",
      "episode :  0\n",
      "current step :  170\n",
      "reward :  -0.5694315320748053\n",
      "episode :  0\n",
      "current step :  171\n",
      "reward :  -0.5481721415158949\n",
      "episode :  0\n",
      "current step :  172\n",
      "reward :  -0.442657554049269\n",
      "episode :  0\n",
      "current step :  173\n",
      "reward :  -0.4801805898109484\n",
      "episode :  0\n",
      "current step :  174\n",
      "reward :  -0.506577124787181\n",
      "episode :  0\n",
      "current step :  175\n",
      "reward :  -0.5371870077295772\n",
      "episode :  0\n",
      "current step :  176\n",
      "reward :  -0.5791197961445163\n",
      "episode :  0\n",
      "current step :  177\n",
      "reward :  -0.6069087182378379\n",
      "episode :  0\n",
      "current step :  178\n",
      "reward :  -0.5493656453980896\n",
      "episode :  0\n",
      "current step :  179\n",
      "reward :  -0.5407213272521345\n",
      "episode :  0\n",
      "current step :  180\n",
      "reward :  -0.5151625858904262\n",
      "episode :  0\n",
      "current step :  181\n",
      "reward :  -0.5942793832947278\n",
      "episode :  0\n",
      "current step :  182\n",
      "reward :  -0.6563701754472882\n",
      "episode :  0\n",
      "current step :  183\n",
      "reward :  -0.6011609332054545\n",
      "episode :  0\n",
      "current step :  184\n",
      "reward :  -0.5690570001916347\n",
      "episode :  0\n",
      "current step :  185\n",
      "reward :  -0.4630393189671274\n",
      "episode :  0\n",
      "current step :  186\n",
      "reward :  -0.5780054732667109\n",
      "episode :  0\n",
      "current step :  187\n",
      "reward :  -0.5395511937824343\n",
      "episode :  0\n",
      "current step :  188\n",
      "reward :  -0.6070234886548096\n",
      "episode :  0\n",
      "current step :  189\n",
      "reward :  -0.5126630872540135\n",
      "episode :  0\n",
      "current step :  190\n",
      "reward :  -0.5757053881747661\n",
      "episode :  0\n",
      "current step :  191\n",
      "reward :  -0.5370710272345524\n",
      "episode :  0\n",
      "current step :  192\n",
      "reward :  -0.5466755828894594\n",
      "episode :  0\n",
      "current step :  193\n",
      "reward :  -0.5735287607646488\n",
      "episode :  0\n",
      "current step :  194\n",
      "reward :  -0.6326942074098824\n",
      "episode :  0\n",
      "current step :  195\n",
      "reward :  -0.5235187533327214\n",
      "episode :  0\n",
      "current step :  196\n",
      "reward :  -0.6168598705338095\n",
      "episode :  0\n",
      "current step :  197\n",
      "reward :  -0.6308362829415396\n",
      "episode :  0\n",
      "current step :  198\n",
      "reward :  -0.5439930353450397\n",
      "episode :  0\n",
      "current step :  199\n",
      "reward :  -0.6040642064905934\n",
      "episode :  0\n",
      "current step :  200\n",
      "reward :  -0.5609958041390208\n",
      "episode :  0\n",
      "current step :  201\n",
      "reward :  -0.6170764133172918\n",
      "episode :  0\n",
      "current step :  202\n",
      "reward :  -0.5849791226715688\n",
      "episode :  0\n",
      "current step :  203\n",
      "reward :  -0.5056633603415975\n",
      "episode :  0\n",
      "current step :  204\n",
      "reward :  -0.530081624128047\n",
      "episode :  0\n",
      "current step :  205\n",
      "reward :  -0.6124326104545614\n",
      "episode :  0\n",
      "current step :  206\n",
      "reward :  -0.5991648319627151\n",
      "episode :  0\n",
      "current step :  207\n",
      "reward :  -0.6035276318456626\n",
      "episode :  0\n",
      "current step :  208\n",
      "reward :  -0.5466262345573617\n",
      "episode :  0\n",
      "current step :  209\n",
      "reward :  -0.5911052227111635\n",
      "episode :  0\n",
      "current step :  210\n",
      "reward :  -0.5749760423482053\n",
      "episode :  0\n",
      "current step :  211\n",
      "reward :  -0.615698247641559\n",
      "episode :  0\n",
      "current step :  212\n",
      "reward :  -0.5647172014241878\n",
      "episode :  0\n",
      "current step :  213\n",
      "reward :  -0.5544565410682109\n",
      "episode :  0\n",
      "current step :  214\n",
      "reward :  -0.5532686828596869\n",
      "episode :  0\n",
      "current step :  215\n",
      "reward :  -0.5764812671659666\n",
      "episode :  0\n",
      "current step :  216\n",
      "reward :  -0.581660048696476\n",
      "episode :  0\n",
      "current step :  217\n",
      "reward :  -0.5864793388025883\n",
      "episode :  0\n",
      "current step :  218\n",
      "reward :  -0.5289284353175145\n",
      "episode :  0\n",
      "current step :  219\n",
      "reward :  -0.5260614410825731\n",
      "episode :  0\n",
      "current step :  220\n",
      "reward :  -0.5117622993528571\n",
      "episode :  0\n",
      "current step :  221\n",
      "reward :  -0.49541386610660193\n",
      "episode :  0\n",
      "current step :  222\n",
      "reward :  -0.5289633934562432\n",
      "episode :  0\n",
      "current step :  223\n",
      "reward :  -0.5932303748965153\n",
      "episode :  0\n",
      "current step :  224\n",
      "reward :  -0.5231576973276261\n",
      "episode :  0\n",
      "current step :  225\n",
      "reward :  -0.5527077286367411\n",
      "episode :  0\n",
      "current step :  226\n",
      "reward :  -0.5442734008429322\n",
      "episode :  0\n",
      "current step :  227\n",
      "reward :  -0.4959007792449641\n",
      "episode :  0\n",
      "current step :  228\n",
      "reward :  -0.5009278520449068\n",
      "episode :  0\n",
      "current step :  229\n",
      "reward :  -0.44136599810809307\n",
      "episode :  0\n",
      "current step :  230\n",
      "reward :  -0.49656724019480364\n",
      "episode :  0\n",
      "current step :  231\n",
      "reward :  -0.44101591722078676\n",
      "episode :  0\n",
      "current step :  232\n",
      "reward :  -0.5138427920236949\n",
      "episode :  0\n",
      "current step :  233\n",
      "reward :  -0.5426109598281426\n",
      "episode :  0\n",
      "current step :  234\n",
      "reward :  -0.46714005406940345\n",
      "episode :  0\n",
      "current step :  235\n",
      "reward :  -0.5481503257892861\n",
      "episode :  0\n",
      "current step :  236\n",
      "reward :  -0.4787804891059529\n",
      "episode :  0\n",
      "current step :  237\n",
      "reward :  -0.4448917145884671\n",
      "episode :  0\n",
      "current step :  238\n",
      "reward :  -0.3403469003534171\n",
      "episode :  0\n",
      "current step :  239\n",
      "reward :  -0.425566034139533\n",
      "episode :  0\n",
      "current step :  240\n",
      "reward :  -0.3825000242248038\n",
      "episode :  0\n",
      "current step :  241\n",
      "reward :  -0.4145987435930955\n",
      "episode :  0\n",
      "current step :  242\n",
      "reward :  -0.35887854231518657\n",
      "episode :  0\n",
      "current step :  243\n",
      "reward :  -0.37693190629198875\n",
      "episode :  0\n",
      "current step :  244\n",
      "reward :  -0.2901517210015165\n",
      "episode :  0\n",
      "current step :  245\n",
      "reward :  -0.36124185378891316\n",
      "episode :  0\n",
      "current step :  246\n",
      "reward :  -0.30193341601407603\n",
      "episode :  0\n",
      "current step :  247\n",
      "reward :  -0.20249625667154333\n",
      "episode :  0\n",
      "current step :  248\n",
      "reward :  -0.21374622956998124\n",
      "episode :  0\n",
      "current step :  249\n",
      "reward :  -0.20016244822596027\n",
      "episode :  0\n",
      "current step :  250\n",
      "reward :  -0.2146306293765293\n",
      "episode :  0\n",
      "current step :  251\n",
      "reward :  -0.20217576269062637\n",
      "episode :  0\n",
      "current step :  252\n",
      "reward :  -0.18967975486222158\n",
      "episode :  0\n",
      "current step :  253\n",
      "reward :  -0.24274496903141143\n",
      "episode :  0\n",
      "current step :  254\n",
      "reward :  -0.19406978792264593\n",
      "episode :  0\n",
      "current step :  255\n",
      "reward :  -0.23910318573617817\n",
      "episode :  0\n",
      "current step :  256\n",
      "reward :  -0.1630617263693\n",
      "episode :  0\n",
      "current step :  257\n",
      "reward :  -0.20144664103865206\n",
      "episode :  0\n",
      "current step :  258\n",
      "reward :  -0.19561044463001018\n",
      "episode :  0\n",
      "current step :  259\n",
      "reward :  -0.20000177418678305\n",
      "episode :  0\n",
      "current step :  260\n",
      "reward :  -0.09698141227186524\n",
      "episode :  0\n",
      "current step :  261\n",
      "reward :  -0.13127226538041017\n",
      "episode :  0\n",
      "current step :  262\n",
      "reward :  -0.177489953270835\n",
      "episode :  0\n",
      "current step :  263\n",
      "reward :  -0.18993479481625045\n",
      "episode :  0\n",
      "current step :  264\n",
      "reward :  -0.1255604170539094\n",
      "episode :  0\n",
      "current step :  265\n",
      "reward :  -0.18781712749877\n",
      "episode :  0\n",
      "current step :  266\n",
      "reward :  -0.2400464996824072\n",
      "episode :  0\n",
      "current step :  267\n",
      "reward :  -0.16702067360079356\n",
      "episode :  0\n",
      "current step :  268\n",
      "reward :  -0.2513682756767817\n",
      "episode :  0\n",
      "current step :  269\n",
      "reward :  -0.14187222875156869\n",
      "episode :  0\n",
      "current step :  270\n",
      "reward :  -0.24698212959169125\n",
      "episode :  0\n",
      "current step :  271\n",
      "reward :  -0.19271491522716203\n",
      "episode :  0\n",
      "current step :  272\n",
      "reward :  -0.17403449492336884\n",
      "episode :  0\n",
      "current step :  273\n",
      "reward :  -0.18832346511165798\n",
      "episode :  0\n",
      "current step :  274\n",
      "reward :  -0.14162440860521316\n",
      "episode :  0\n",
      "current step :  275\n",
      "reward :  -0.15488067681648135\n",
      "episode :  0\n",
      "current step :  276\n",
      "reward :  -0.173588930383574\n",
      "episode :  0\n",
      "current step :  277\n",
      "reward :  -0.14282034639552532\n",
      "episode :  0\n",
      "current step :  278\n",
      "reward :  -0.15995849224979133\n",
      "episode :  0\n",
      "current step :  279\n",
      "reward :  -0.22889395088183917\n",
      "episode :  0\n",
      "current step :  280\n",
      "reward :  -0.14724710765393342\n",
      "episode :  0\n",
      "current step :  281\n",
      "reward :  -0.18153701738036693\n",
      "episode :  0\n",
      "current step :  282\n",
      "reward :  -0.18838121981979306\n",
      "episode :  0\n",
      "current step :  283\n",
      "reward :  -0.16273328355962202\n",
      "episode :  0\n",
      "current step :  284\n",
      "reward :  -0.13187834352286998\n",
      "episode :  0\n",
      "current step :  285\n",
      "reward :  -0.15762954411709673\n",
      "episode :  1\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "model.learn(total_timesteps= 3, log_interval=10)\n",
    "model.save(\"ddpg_imitation\")\n",
    "vec_env = model.get_env()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9265cdc2",
   "metadata": {},
   "source": [
    "For training, we can stop here. We can begin the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fb9538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step :  0\n",
      "reward :  -0.28889508076508236\n",
      "episode :  1\n",
      "current step :  1\n",
      "reward :  -0.2811715053996018\n",
      "episode :  1\n",
      "current step :  2\n",
      "reward :  -0.27503347737146683\n",
      "episode :  1\n",
      "current step :  3\n",
      "reward :  -0.26890865742018255\n",
      "episode :  1\n",
      "current step :  4\n",
      "reward :  -0.26421023431067325\n",
      "episode :  1\n",
      "current step :  5\n",
      "reward :  -0.261199664732106\n",
      "episode :  1\n",
      "current step :  6\n",
      "reward :  -0.25920848924129003\n",
      "episode :  1\n",
      "current step :  7\n",
      "reward :  -0.256449109483863\n",
      "episode :  1\n",
      "current step :  8\n",
      "reward :  -0.25557050504094386\n",
      "episode :  1\n",
      "current step :  9\n",
      "reward :  -0.26077953732745796\n",
      "episode :  1\n",
      "current step :  10\n",
      "reward :  -0.26683958146265463\n",
      "episode :  1\n",
      "current step :  11\n",
      "reward :  -0.2749114427359574\n",
      "episode :  1\n",
      "current step :  12\n",
      "reward :  -0.28225579299478165\n",
      "episode :  1\n",
      "current step :  13\n",
      "reward :  -0.28658717676685325\n",
      "episode :  1\n",
      "current step :  14\n",
      "reward :  -0.29352257843150475\n",
      "episode :  1\n",
      "current step :  15\n",
      "reward :  -0.298516703232687\n",
      "episode :  1\n",
      "current step :  16\n",
      "reward :  -0.3026783230236842\n",
      "episode :  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mtargets:\n\u001b[0;32m      8\u001b[0m     action, _states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39marray(t\u001b[39m.\u001b[39mflatten())\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m     obs, rewards, dones, info \u001b[39m=\u001b[39m vec_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\gym-examples\\gym_examples\\envs\\Poppy_Env.py:88\u001b[0m, in \u001b[0;36mPoppyEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m k,m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoppy\u001b[39m.\u001b[39mr_arm_chain\u001b[39m.\u001b[39mmotors):\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m (m\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mabs_z\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (m\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbust_y\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (m\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbust_x\u001b[39m\u001b[39m'\u001b[39m):   \n\u001b[1;32m---> 88\u001b[0m                 m\u001b[39m.\u001b[39;49mgoto_position(action_r[k], \u001b[39m1\u001b[39;49m, wait\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     90\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m                 m\u001b[39m.\u001b[39mgoto_position(\u001b[39m0.0\u001b[39m, \u001b[39m1\u001b[39m, wait\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)             \n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\dynamixel\\motor.py:263\u001b[0m, in \u001b[0;36mDxlMotor.goto_position\u001b[1;34m(self, position, duration, control, wait)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal_position \u001b[39m=\u001b[39m position\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m--> 263\u001b[0m         time\u001b[39m.\u001b[39;49msleep(duration)\n\u001b[0;32m    265\u001b[0m \u001b[39melif\u001b[39;00m control \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    266\u001b[0m     goto_linear \u001b[39m=\u001b[39m GotoLinear(\u001b[39mself\u001b[39m, position, duration)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\vrep\\__init__.py:49\u001b[0m, in \u001b[0;36mvrep_time.sleep\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_time() \u001b[39m<\u001b[39m t0:\n\u001b[0;32m     48\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m sys_time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DDPG.load(\"ddpg_imitation\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3812a3d7",
   "metadata": {},
   "source": [
    "The below is just some other tials, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba77d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "\n",
    "model = DDPG.load(\"ddpg_imitation\")\n",
    "model.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3088d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = env.step(action.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314bfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
