{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c47c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "\n",
    "import sys\n",
    "sys.path.append('gym-examples')\n",
    "import gym_examples\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238796fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Poppy!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_examples/Poppy-v0') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a464c7fb",
   "metadata": {},
   "source": [
    "Check your CoppeliaSim, there is now a Poppy Torso on a table. \n",
    "\n",
    "Remove that table !\n",
    "\n",
    "Simply click on it and press the delete key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be88176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.114035  ,  0.03490575, -0.10615204, -0.10555284,  0.00190347,\n",
       "       -0.10847506], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8c2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "action: [157.75662     3.1193018]\n",
      "current step :  0\n",
      "reward :  -0.35357784369616646\n",
      "episode :  0\n",
      "action: [97.45637  14.720977]\n",
      "current step :  1\n",
      "reward :  -0.4474039463919798\n",
      "episode :  0\n",
      "action: [116.53238  42.91779]\n",
      "current step :  2\n",
      "reward :  -0.518902710080403\n",
      "episode :  0\n",
      "action: [40.366505 22.883545]\n",
      "current step :  3\n",
      "reward :  -0.3310534528136335\n",
      "episode :  0\n",
      "action: [155.48853   12.854584]\n",
      "current step :  4\n",
      "reward :  -0.4682480171474222\n",
      "episode :  0\n",
      "action: [116.75309   65.358696]\n",
      "current step :  5\n",
      "reward :  -0.5015506526699244\n",
      "episode :  0\n",
      "action: [126.076584  88.2248  ]\n",
      "current step :  6\n",
      "reward :  -0.5425711688748246\n",
      "episode :  0\n",
      "action: [35.351974 28.258284]\n",
      "current step :  7\n",
      "reward :  -0.3294660622105196\n",
      "episode :  0\n",
      "action: [ 93.165596 177.33487 ]\n",
      "current step :  8\n",
      "reward :  -0.46994898252997946\n",
      "episode :  0\n",
      "action: [ 19.706738 157.59462 ]\n",
      "current step :  9\n",
      "reward :  -0.31356488088948536\n",
      "episode :  0\n",
      "action: [37.178    32.438892]\n",
      "current step :  10\n",
      "reward :  -0.338753536694481\n",
      "episode :  0\n",
      "action: [120.29163  68.31983]\n",
      "current step :  11\n",
      "reward :  -0.513728331209523\n",
      "episode :  0\n",
      "action: [160.58975   12.678051]\n",
      "current step :  12\n",
      "reward :  -0.6157944975423514\n",
      "episode :  0\n",
      "action: [134.26472 111.37253]\n",
      "current step :  13\n",
      "reward :  -0.5700250231590845\n",
      "episode :  0\n",
      "action: [ 43.38995 170.47418]\n",
      "current step :  14\n",
      "reward :  -0.36422002817046245\n",
      "episode :  0\n",
      "action: [29.542671 48.21124 ]\n",
      "current step :  15\n",
      "reward :  -0.3649695648196249\n",
      "episode :  0\n",
      "action: [107.92189 141.02885]\n",
      "current step :  16\n",
      "reward :  -0.5272402281555221\n",
      "episode :  0\n",
      "action: [ 12.099509 115.11555 ]\n",
      "current step :  17\n",
      "reward :  -0.38138105188857546\n",
      "episode :  0\n",
      "action: [143.377     6.25504]\n",
      "current step :  18\n",
      "reward :  -0.45540916197758086\n",
      "episode :  0\n",
      "action: [120.8408   87.43909]\n",
      "current step :  19\n",
      "reward :  -0.5944844278726136\n",
      "episode :  0\n",
      "action: [ 36.18507 129.99997]\n",
      "current step :  20\n",
      "reward :  -0.4275863708402066\n",
      "episode :  0\n",
      "action: [156.44635 177.31537]\n",
      "current step :  21\n",
      "reward :  -0.5638240245190803\n",
      "episode :  0\n",
      "action: [153.82968   43.161205]\n",
      "current step :  22\n",
      "reward :  -0.6913174777175752\n",
      "episode :  0\n",
      "action: [  4.6470933 131.83847  ]\n",
      "current step :  23\n",
      "reward :  -0.4680639035464196\n",
      "episode :  0\n",
      "action: [135.61028 136.75764]\n",
      "current step :  24\n",
      "reward :  -0.5244379923324964\n",
      "episode :  0\n",
      "action: [109.86008   21.367561]\n",
      "current step :  25\n",
      "reward :  -0.6431167233303657\n",
      "episode :  0\n",
      "action: [174.2772  162.79836]\n",
      "current step :  26\n",
      "reward :  -0.7519932082279478\n",
      "episode :  0\n",
      "action: [148.70499 108.30711]\n",
      "current step :  27\n",
      "reward :  -0.7454569985041957\n",
      "episode :  0\n",
      "action: [  2.8660102 151.57898  ]\n",
      "current step :  28\n",
      "reward :  -0.5635964002087108\n",
      "episode :  0\n",
      "action: [52.204704 38.0182  ]\n",
      "current step :  29\n",
      "reward :  -0.5785242502674972\n",
      "episode :  0\n",
      "action: [24.957918 33.559082]\n",
      "current step :  30\n",
      "reward :  -0.5513763980522183\n",
      "episode :  0\n",
      "action: [34.97602  21.396303]\n",
      "current step :  31\n",
      "reward :  -0.5738176090978813\n",
      "episode :  0\n",
      "action: [50.319103 33.324436]\n",
      "current step :  32\n",
      "reward :  -0.604151175160742\n",
      "episode :  0\n",
      "action: [76.830284 26.217756]\n",
      "current step :  33\n",
      "reward :  -0.6694507409967198\n",
      "episode :  0\n",
      "action: [ 21.128017 100.67722 ]\n",
      "current step :  34\n",
      "reward :  -0.5781438301715145\n",
      "episode :  0\n",
      "action: [161.25656 173.30669]\n",
      "current step :  35\n",
      "reward :  -0.7249317938398112\n",
      "episode :  0\n",
      "action: [ 81.509705 150.93774 ]\n",
      "current step :  36\n",
      "reward :  -0.6735266846702154\n",
      "episode :  0\n",
      "action: [49.89679 79.67288]\n",
      "current step :  37\n",
      "reward :  -0.6240487035324065\n",
      "episode :  0\n",
      "action: [85.98621 91.99756]\n",
      "current step :  38\n",
      "reward :  -0.7169738082795043\n",
      "episode :  0\n",
      "action: [89.80263    6.7756996]\n",
      "current step :  39\n",
      "reward :  -0.7137684437429299\n",
      "episode :  0\n",
      "action: [76.61257  77.620544]\n",
      "current step :  40\n",
      "reward :  -0.6777701403033012\n",
      "episode :  0\n",
      "action: [ 0.5024314 27.551125 ]\n",
      "current step :  41\n",
      "reward :  -0.577374148364882\n",
      "episode :  0\n",
      "action: [62.723175 41.696255]\n",
      "current step :  42\n",
      "reward :  -0.6538988833027399\n",
      "episode :  0\n",
      "action: [42.42451  63.300037]\n",
      "current step :  43\n",
      "reward :  -0.6077988311270509\n",
      "episode :  0\n",
      "action: [57.8067   23.832897]\n",
      "current step :  44\n",
      "reward :  -0.6384400295188214\n",
      "episode :  0\n",
      "action: [103.88996  70.26779]\n",
      "current step :  45\n",
      "reward :  -0.7566399273623249\n",
      "episode :  0\n",
      "action: [128.35889 131.92303]\n",
      "current step :  46\n",
      "reward :  -0.8080612407177088\n",
      "episode :  0\n",
      "action: [94.899   44.01279]\n",
      "current step :  47\n",
      "reward :  -0.7222686567836426\n",
      "episode :  0\n",
      "action: [144.92253 169.57468]\n",
      "current step :  48\n",
      "reward :  -0.839096725174991\n",
      "episode :  0\n",
      "action: [149.6077   105.523926]\n",
      "current step :  49\n",
      "reward :  -0.8411903875128419\n",
      "episode :  0\n",
      "action: [118.44448 112.56253]\n",
      "current step :  50\n",
      "reward :  -0.7795426314724214\n",
      "episode :  0\n",
      "action: [104.69347   24.396301]\n",
      "current step :  51\n",
      "reward :  -0.7595395822132373\n",
      "episode :  0\n",
      "action: [79.99282 98.44929]\n",
      "current step :  52\n",
      "reward :  -0.6970945662576487\n",
      "episode :  0\n",
      "action: [143.03606  32.52201]\n",
      "current step :  53\n",
      "reward :  -0.8324848838915071\n",
      "episode :  0\n",
      "action: [ 6.7703676 80.92093  ]\n",
      "current step :  54\n",
      "reward :  -0.5901944553550726\n",
      "episode :  0\n",
      "action: [ 70.026306 153.87387 ]\n",
      "current step :  55\n",
      "reward :  -0.6960566543934146\n",
      "episode :  0\n",
      "action: [104.0216  176.55139]\n",
      "current step :  56\n",
      "reward :  -0.7675921775225634\n",
      "episode :  0\n",
      "action: [120.63104   81.961044]\n",
      "current step :  57\n",
      "reward :  -0.8039230110256843\n",
      "episode :  0\n",
      "action: [ 86.57259 176.93895]\n",
      "current step :  58\n",
      "reward :  -0.7145988134002079\n",
      "episode :  0\n",
      "action: [115.52877  57.06153]\n",
      "current step :  59\n",
      "reward :  -0.8020381976703231\n",
      "episode :  0\n",
      "action: [ 66.343094 151.04254 ]\n",
      "current step :  60\n",
      "reward :  -0.6745281073363332\n",
      "episode :  0\n",
      "action: [155.48836 146.29321]\n",
      "current step :  61\n",
      "reward :  -0.8194006551827636\n",
      "episode :  0\n",
      "action: [0.27083337 2.8827202 ]\n",
      "current step :  62\n",
      "reward :  -0.6162920379015006\n",
      "episode :  0\n",
      "action: [85.88053 85.57541]\n",
      "current step :  63\n",
      "reward :  -0.7331272285943256\n",
      "episode :  0\n",
      "action: [166.94019  119.809875]\n",
      "current step :  64\n",
      "reward :  -0.850834638536775\n",
      "episode :  0\n",
      "action: [155.2964   69.27312]\n",
      "current step :  65\n",
      "reward :  -0.8608075231319062\n",
      "episode :  0\n",
      "action: [138.38646 167.33781]\n",
      "current step :  66\n",
      "reward :  -0.8368430241502064\n",
      "episode :  0\n",
      "action: [52.90721  29.341413]\n",
      "current step :  67\n",
      "reward :  -0.6745585513269504\n",
      "episode :  0\n",
      "action: [26.07517  59.843906]\n",
      "current step :  68\n",
      "reward :  -0.6074646061965171\n",
      "episode :  0\n",
      "action: [ 8.9447  66.24815]\n",
      "current step :  69\n",
      "reward :  -0.5952794671796777\n",
      "episode :  0\n",
      "action: [145.72855 132.70274]\n",
      "current step :  70\n",
      "reward :  -0.7006707281986249\n",
      "episode :  0\n",
      "action: [100.34901 159.64203]\n",
      "current step :  71\n",
      "reward :  -0.7572667609115019\n",
      "episode :  0\n",
      "action: [ 68.9193  148.69214]\n",
      "current step :  72\n",
      "reward :  -0.681999147228773\n",
      "episode :  0\n",
      "action: [128.48962 115.40591]\n",
      "current step :  73\n",
      "reward :  -0.7756062649887918\n",
      "episode :  0\n",
      "action: [ 25.812458 129.29895 ]\n",
      "current step :  74\n",
      "reward :  -0.6013199192867443\n",
      "episode :  0\n",
      "action: [159.31117  87.13085]\n",
      "current step :  75\n",
      "reward :  -0.7370183318757647\n",
      "episode :  0\n",
      "action: [176.27052   38.310463]\n",
      "current step :  76\n",
      "reward :  -0.853958842334504\n",
      "episode :  0\n",
      "action: [ 1.5920949 39.041924 ]\n",
      "current step :  77\n",
      "reward :  -0.6906672211047399\n",
      "episode :  0\n",
      "action: [166.31789     7.1499805]\n",
      "current step :  78\n",
      "reward :  -0.6418138489405631\n",
      "episode :  0\n",
      "action: [ 2.2383463 62.148613 ]\n",
      "current step :  79\n",
      "reward :  -0.5869940060843146\n",
      "episode :  0\n",
      "action: [156.82443 135.18436]\n",
      "current step :  80\n",
      "reward :  -0.6690957870849963\n",
      "episode :  0\n",
      "action: [  3.8608627 141.33366  ]\n",
      "current step :  81\n",
      "reward :  -0.6542603071587519\n",
      "episode :  0\n",
      "action: [110.44099   75.058075]\n",
      "current step :  82\n",
      "reward :  -0.6941522196017172\n",
      "episode :  0\n",
      "action: [  7.5371037 134.19444  ]\n",
      "current step :  83\n",
      "reward :  -0.5559375195923547\n",
      "episode :  0\n",
      "action: [  6.260324 100.94792 ]\n",
      "current step :  84\n",
      "reward :  -0.5505286724712251\n",
      "episode :  0\n",
      "action: [41.450966 75.63899 ]\n",
      "current step :  85\n",
      "reward :  -0.5763118889318745\n",
      "episode :  0\n",
      "action: [58.196556 28.458248]\n",
      "current step :  86\n",
      "reward :  -0.6021560516370659\n",
      "episode :  0\n",
      "action: [ 41.013023 106.14014 ]\n",
      "current step :  87\n",
      "reward :  -0.5616381194878105\n",
      "episode :  0\n",
      "action: [126.27087 132.82556]\n",
      "current step :  88\n",
      "reward :  -0.6884178995863908\n",
      "episode :  0\n",
      "action: [170.75945  67.85538]\n",
      "current step :  89\n",
      "reward :  -0.7769085691288001\n",
      "episode :  0\n",
      "action: [ 68.99064 160.10046]\n",
      "current step :  90\n",
      "reward :  -0.5931128560409227\n",
      "episode :  0\n",
      "action: [ 88.72198 134.6768 ]\n",
      "current step :  91\n",
      "reward :  -0.6182524812284176\n",
      "episode :  0\n",
      "action: [65.79949 79.19539]\n",
      "current step :  92\n",
      "reward :  -0.5553383181770319\n",
      "episode :  0\n",
      "action: [22.545334 70.868004]\n",
      "current step :  93\n",
      "reward :  -0.47898570161698906\n",
      "episode :  0\n",
      "action: [141.78107  70.27082]\n",
      "current step :  94\n",
      "reward :  -0.5910988928600415\n",
      "episode :  0\n",
      "action: [132.94522  58.66042]\n",
      "current step :  95\n",
      "reward :  -0.66520351845551\n",
      "episode :  0\n",
      "action: [173.39551 170.26624]\n",
      "current step :  96\n",
      "reward :  -0.7000960843919788\n",
      "episode :  0\n",
      "action: [173.67552 124.91788]\n",
      "current step :  97\n",
      "reward :  -0.6870694437820511\n",
      "episode :  0\n",
      "action: [ 17.03257 143.42188]\n",
      "current step :  98\n",
      "reward :  -0.4927225592196332\n",
      "episode :  0\n",
      "action: [ 1.3283747 26.16447  ]\n",
      "current step :  99\n",
      "reward :  -0.3956251341737251\n",
      "episode :  0\n",
      "action: [40.38561 21.30189]\n",
      "current step :  100\n",
      "reward :  -0.40998916793754975\n",
      "episode :  0\n",
      "action: [176.32333 125.15842]\n",
      "current step :  101\n",
      "reward :  -0.5186217437030072\n",
      "episode :  0\n",
      "action: [107.82411  72.397  ]\n",
      "current step :  102\n",
      "reward :  -0.5163352830523132\n",
      "episode :  0\n",
      "action: [26.800268 16.521984]\n",
      "current step :  103\n",
      "reward :  -0.3449479984957274\n",
      "episode :  0\n",
      "action: [18.260822 47.673153]\n",
      "current step :  104\n",
      "reward :  -0.3404737183325034\n",
      "episode :  0\n",
      "action: [80.34536  88.937325]\n",
      "current step :  105\n",
      "reward :  -0.4509776270156384\n",
      "episode :  0\n",
      "action: [45.858055 66.4146  ]\n",
      "current step :  106\n",
      "reward :  -0.34908217935732044\n",
      "episode :  0\n",
      "action: [104.35833 138.22107]\n",
      "current step :  107\n",
      "reward :  -0.498566622334852\n",
      "episode :  0\n",
      "action: [ 57.377712 134.2261  ]\n",
      "current step :  108\n",
      "reward :  -0.35523596069253327\n",
      "episode :  0\n",
      "action: [ 66.69338 118.48631]\n",
      "current step :  109\n",
      "reward :  -0.38686735567928004\n",
      "episode :  0\n",
      "action: [178.60309  19.34273]\n",
      "current step :  110\n",
      "reward :  -0.5808518803678618\n",
      "episode :  0\n",
      "action: [143.60757 115.68051]\n",
      "current step :  111\n",
      "reward :  -0.5679476810144334\n",
      "episode :  0\n",
      "action: [41.10689 96.69314]\n",
      "current step :  112\n",
      "reward :  -0.32688660085128957\n",
      "episode :  0\n",
      "action: [ 40.08158 171.83186]\n",
      "current step :  113\n",
      "reward :  -0.3119192895802841\n",
      "episode :  0\n",
      "action: [ 44.07584 158.98691]\n",
      "current step :  114\n",
      "reward :  -0.3162831256119113\n",
      "episode :  0\n",
      "action: [ 55.915474 150.81879 ]\n",
      "current step :  115\n",
      "reward :  -0.34306023614731973\n",
      "episode :  0\n",
      "action: [124.13751  93.69248]\n",
      "current step :  116\n",
      "reward :  -0.5387263935965743\n",
      "episode :  0\n",
      "action: [ 49.957893 150.29782 ]\n",
      "current step :  117\n",
      "reward :  -0.3088543984038661\n",
      "episode :  0\n",
      "action: [165.05136     3.8547902]\n",
      "current step :  118\n",
      "reward :  -0.5019327629252728\n",
      "episode :  0\n",
      "action: [ 75.42358 136.44292]\n",
      "current step :  119\n",
      "reward :  -0.39315903531467367\n",
      "episode :  0\n",
      "action: [ 64.352646 112.66065 ]\n",
      "current step :  120\n",
      "reward :  -0.3601384896780863\n",
      "episode :  0\n",
      "action: [177.13036  89.95634]\n",
      "current step :  121\n",
      "reward :  -0.48490917862840577\n",
      "episode :  0\n",
      "action: [90.639435   7.3646917]\n",
      "current step :  122\n",
      "reward :  -0.4514585425574818\n",
      "episode :  0\n",
      "action: [ 28.153893 114.64791 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m SAC(\u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m284\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m16\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39msac_imitation\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:302\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    294\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[0;32m    295\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    300\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    303\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    304\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    305\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    306\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    307\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    308\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    309\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    308\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    310\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 311\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    313\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    314\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    315\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    316\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    317\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:543\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    540\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    542\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    546\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\gym-examples\\gym_examples\\envs\\Poppy_Env.py:104\u001b[0m, in \u001b[0;36mPoppyEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    102\u001b[0m         m\u001b[39m.\u001b[39mgoto_position(\u001b[39m90.0\u001b[39m, \u001b[39m0.01\u001b[39m, wait\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    103\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m         m\u001b[39m.\u001b[39;49mgoto_position(\u001b[39m0.0\u001b[39;49m, \u001b[39m0.01\u001b[39;49m, wait\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m k,m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoppy\u001b[39m.\u001b[39mr_arm_chain\u001b[39m.\u001b[39mmotors):\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr_shoulder_x\u001b[39m\u001b[39m'\u001b[39m:   \n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\dynamixel\\motor.py:263\u001b[0m, in \u001b[0;36mDxlMotor.goto_position\u001b[1;34m(self, position, duration, control, wait)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal_position \u001b[39m=\u001b[39m position\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m--> 263\u001b[0m         time\u001b[39m.\u001b[39;49msleep(duration)\n\u001b[0;32m    265\u001b[0m \u001b[39melif\u001b[39;00m control \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    266\u001b[0m     goto_linear \u001b[39m=\u001b[39m GotoLinear(\u001b[39mself\u001b[39m, position, duration)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\vrep\\__init__.py:47\u001b[0m, in \u001b[0;36mvrep_time.sleep\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     45\u001b[0m t0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_time()\n\u001b[0;32m     46\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_time() \u001b[39m-\u001b[39m t0) \u001b[39m<\u001b[39m t:\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_time() \u001b[39m<\u001b[39m t0:\n\u001b[0;32m     48\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     sys_time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\vrep\\__init__.py:33\u001b[0m, in \u001b[0;36mvrep_time.get_time\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m     sys_time\u001b[39m.\u001b[39msleep(\u001b[39m.5\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_time(trial \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m trial \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not get current simulation time. Make sure the V-REP simulation is running. And that you have added the \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m child script to your scene.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\vrep\\__init__.py:29\u001b[0m, in \u001b[0;36mvrep_time.get_time\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_time\u001b[39m(\u001b[39mself\u001b[39m, trial\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mget_simulation_current_time()\n\u001b[0;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m         sys_time\u001b[39m.\u001b[39msleep(\u001b[39m.5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\vrep\\io.py:227\u001b[0m, in \u001b[0;36mVrepIO.get_simulation_current_time\u001b[1;34m(self, timer)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Gets the simulation current time. \"\"\"\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_remote_api(\u001b[39m'\u001b[39;49m\u001b[39msimxGetFloatSignal\u001b[39;49m\u001b[39m'\u001b[39;49m, timer, streaming\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    228\u001b[0m \u001b[39mexcept\u001b[39;00m VrepIOErrors:\n\u001b[0;32m    229\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Joffrey\\Projet\\MSIA\\RL\\poppy-torso-track\\venv\\lib\\site-packages\\pypot\\vrep\\io.py:323\u001b[0m, in \u001b[0;36mVrepIO.call_remote_api\u001b[1;34m(self, func_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m remote_api\u001b[39m.\u001b[39msimx_return_novalue_flag \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m err:\n\u001b[0;32m    321\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     time\u001b[39m.\u001b[39;49msleep(VrepIO\u001b[39m.\u001b[39;49mTIMEOUT)\n\u001b[0;32m    325\u001b[0m \u001b[39m# if any(err) and hard_retry:\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m#     print \"HARD RETRY\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m# self.stop_simulation() #nope\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39m#         return res\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(err):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SAC('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=284*16)\n",
    "model.save(\"sac_imitation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3812a3d7",
   "metadata": {},
   "source": [
    "The below is just some other tials, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede5ffcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01490104,  0.2733797 ,  0.0707105 , -0.03437201, -0.05124207,\n",
       "        0.17512381], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c314bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step :  0\n",
      "reward :  -0.2673593408203816\n",
      "episode :  0\n",
      "current step :  1\n",
      "reward :  -0.4737529583374882\n",
      "episode :  0\n",
      "current step :  2\n",
      "reward :  -0.34254982554339836\n",
      "episode :  0\n",
      "current step :  3\n",
      "reward :  -0.5634396858778961\n",
      "episode :  0\n",
      "current step :  4\n",
      "reward :  -0.48636238019823774\n",
      "episode :  0\n",
      "current step :  5\n",
      "reward :  -0.5507958105241483\n",
      "episode :  0\n",
      "current step :  6\n",
      "reward :  -0.4924831474281916\n",
      "episode :  0\n",
      "current step :  7\n",
      "reward :  -0.2703698988052062\n",
      "episode :  0\n",
      "current step :  8\n",
      "reward :  -0.37861910230903123\n",
      "episode :  0\n",
      "current step :  9\n",
      "reward :  -0.538798896394975\n",
      "episode :  0\n",
      "current step :  10\n",
      "reward :  -0.5102274525866564\n",
      "episode :  0\n",
      "current step :  11\n",
      "reward :  -0.4826531076924136\n",
      "episode :  0\n",
      "current step :  12\n",
      "reward :  -0.3948551383191523\n",
      "episode :  0\n",
      "current step :  13\n",
      "reward :  -0.4988508790505991\n",
      "episode :  0\n",
      "current step :  14\n",
      "reward :  -0.21205404658521496\n",
      "episode :  0\n",
      "current step :  15\n",
      "reward :  -0.46882035472781136\n",
      "episode :  0\n",
      "current step :  16\n",
      "reward :  -0.28638086322313244\n",
      "episode :  0\n",
      "current step :  17\n",
      "reward :  -0.4992508649160702\n",
      "episode :  0\n",
      "current step :  18\n",
      "reward :  -0.48585080597186175\n",
      "episode :  0\n",
      "current step :  19\n",
      "reward :  -0.5939320652696163\n",
      "episode :  0\n",
      "current step :  20\n",
      "reward :  -0.4005171312728929\n",
      "episode :  0\n",
      "current step :  21\n",
      "reward :  -0.431072724150172\n",
      "episode :  0\n",
      "current step :  22\n",
      "reward :  -0.5833101027858448\n",
      "episode :  0\n",
      "current step :  23\n",
      "reward :  -0.5204501735035935\n",
      "episode :  0\n",
      "current step :  24\n",
      "reward :  -0.5375040797828552\n",
      "episode :  0\n",
      "current step :  25\n",
      "reward :  -0.2955614489212488\n",
      "episode :  0\n",
      "current step :  26\n",
      "reward :  -0.5575623704202366\n",
      "episode :  0\n",
      "current step :  27\n",
      "reward :  -0.28407823331768123\n",
      "episode :  0\n",
      "current step :  28\n",
      "reward :  -0.6185458678676304\n",
      "episode :  0\n",
      "current step :  29\n",
      "reward :  -0.6127777027855184\n",
      "episode :  0\n",
      "current step :  30\n",
      "reward :  -0.5739462994964055\n",
      "episode :  0\n",
      "current step :  31\n",
      "reward :  -0.6506937302651649\n",
      "episode :  0\n",
      "current step :  32\n",
      "reward :  -0.4519695689571161\n",
      "episode :  0\n",
      "current step :  33\n",
      "reward :  -0.48525168341415786\n",
      "episode :  0\n",
      "current step :  34\n",
      "reward :  -0.4101020753086571\n",
      "episode :  0\n",
      "current step :  35\n",
      "reward :  -0.5183691151631635\n",
      "episode :  0\n",
      "current step :  36\n",
      "reward :  -0.4745437053109196\n",
      "episode :  0\n",
      "current step :  37\n",
      "reward :  -0.4639754370673978\n",
      "episode :  0\n",
      "current step :  38\n",
      "reward :  -0.39727613948511054\n",
      "episode :  0\n",
      "current step :  39\n",
      "reward :  -0.3155271321662458\n",
      "episode :  0\n",
      "current step :  40\n",
      "reward :  -0.38380668506727467\n",
      "episode :  0\n",
      "current step :  41\n",
      "reward :  -0.3523696868204528\n",
      "episode :  0\n",
      "current step :  42\n",
      "reward :  -0.3433605967458722\n",
      "episode :  0\n",
      "current step :  43\n",
      "reward :  -0.4812965452274364\n",
      "episode :  0\n",
      "current step :  44\n",
      "reward :  -0.4865569271786752\n",
      "episode :  0\n",
      "current step :  45\n",
      "reward :  -0.6606291023029534\n",
      "episode :  0\n",
      "current step :  46\n",
      "reward :  -0.4763611641109923\n",
      "episode :  0\n",
      "current step :  47\n",
      "reward :  -0.47799420026755113\n",
      "episode :  0\n",
      "current step :  48\n",
      "reward :  -0.48406738478282785\n",
      "episode :  0\n",
      "current step :  49\n",
      "reward :  -0.35360992683723175\n",
      "episode :  0\n",
      "current step :  50\n",
      "reward :  -0.3931423998638771\n",
      "episode :  0\n",
      "current step :  51\n",
      "reward :  -0.4702008266213711\n",
      "episode :  0\n",
      "current step :  52\n",
      "reward :  -0.5403099079812766\n",
      "episode :  0\n",
      "current step :  53\n",
      "reward :  -0.1654847821361723\n",
      "episode :  0\n",
      "current step :  54\n",
      "reward :  -0.3463498714691733\n",
      "episode :  0\n",
      "current step :  55\n",
      "reward :  -0.44771097452031866\n",
      "episode :  0\n",
      "current step :  56\n",
      "reward :  -0.48088454067723996\n",
      "episode :  0\n",
      "current step :  57\n",
      "reward :  -0.7294169426011793\n",
      "episode :  0\n",
      "current step :  58\n",
      "reward :  -0.37800873833403\n",
      "episode :  0\n",
      "current step :  59\n",
      "reward :  -0.4851845648650076\n",
      "episode :  0\n",
      "current step :  60\n",
      "reward :  -0.4699735745699158\n",
      "episode :  0\n",
      "current step :  61\n",
      "reward :  -0.7118809473972938\n",
      "episode :  0\n",
      "current step :  62\n",
      "reward :  -0.4026599765554306\n",
      "episode :  0\n",
      "current step :  63\n",
      "reward :  -0.4799479365845659\n",
      "episode :  0\n",
      "current step :  64\n",
      "reward :  -0.380043725674825\n",
      "episode :  0\n",
      "current step :  65\n",
      "reward :  -0.6703831352208838\n",
      "episode :  0\n",
      "current step :  66\n",
      "reward :  -0.39399445528160865\n",
      "episode :  0\n",
      "current step :  67\n",
      "reward :  -0.35903630666904435\n",
      "episode :  0\n",
      "current step :  68\n",
      "reward :  -0.49259211300061223\n",
      "episode :  0\n",
      "current step :  69\n",
      "reward :  -0.4569840687957224\n",
      "episode :  0\n",
      "current step :  70\n",
      "reward :  -0.3789905630558922\n",
      "episode :  0\n",
      "current step :  71\n",
      "reward :  -0.3169686315002082\n",
      "episode :  0\n",
      "current step :  72\n",
      "reward :  -0.5119640450015598\n",
      "episode :  0\n",
      "current step :  73\n",
      "reward :  -0.3756159229370283\n",
      "episode :  0\n",
      "current step :  74\n",
      "reward :  -0.5216594366335151\n",
      "episode :  0\n",
      "current step :  75\n",
      "reward :  -0.5688478405705276\n",
      "episode :  0\n",
      "current step :  76\n",
      "reward :  -0.5092292940472536\n",
      "episode :  0\n",
      "current step :  77\n",
      "reward :  -0.4048433448983445\n",
      "episode :  0\n",
      "current step :  78\n",
      "reward :  -0.45366632244017224\n",
      "episode :  0\n",
      "current step :  79\n",
      "reward :  -0.643615440128453\n",
      "episode :  0\n",
      "current step :  80\n",
      "reward :  -0.4936390684493386\n",
      "episode :  0\n",
      "current step :  81\n",
      "reward :  -0.4919925594954978\n",
      "episode :  0\n",
      "current step :  82\n",
      "reward :  -0.2402372981988626\n",
      "episode :  0\n",
      "current step :  83\n",
      "reward :  -0.5619273478086195\n",
      "episode :  0\n",
      "current step :  84\n",
      "reward :  -0.3035542710659145\n",
      "episode :  0\n",
      "current step :  85\n",
      "reward :  -0.2734312933025132\n",
      "episode :  0\n",
      "current step :  86\n",
      "reward :  -0.47554447655768367\n",
      "episode :  0\n",
      "current step :  87\n",
      "reward :  -0.5698694723172629\n",
      "episode :  0\n",
      "current step :  88\n",
      "reward :  -0.4197464519038756\n",
      "episode :  0\n",
      "current step :  89\n",
      "reward :  -0.39839566712862223\n",
      "episode :  0\n",
      "current step :  90\n",
      "reward :  -0.45080287282107123\n",
      "episode :  0\n",
      "current step :  91\n",
      "reward :  -0.45670223083911143\n",
      "episode :  0\n",
      "current step :  92\n",
      "reward :  -0.4844563652366415\n",
      "episode :  0\n",
      "current step :  93\n",
      "reward :  -0.3929519108578815\n",
      "episode :  0\n",
      "current step :  94\n",
      "reward :  -0.45934220941997533\n",
      "episode :  0\n",
      "current step :  95\n",
      "reward :  -0.5567334280909924\n",
      "episode :  0\n",
      "current step :  96\n",
      "reward :  -0.5103871996561407\n",
      "episode :  0\n",
      "current step :  97\n",
      "reward :  -0.5639320289682186\n",
      "episode :  0\n",
      "current step :  98\n",
      "reward :  -0.5088791711623559\n",
      "episode :  0\n",
      "current step :  99\n",
      "reward :  -0.5778100414488665\n",
      "episode :  0\n",
      "current step :  100\n",
      "reward :  -0.5227135364287804\n",
      "episode :  0\n",
      "current step :  101\n",
      "reward :  -0.5091134057939681\n",
      "episode :  0\n",
      "current step :  102\n",
      "reward :  -0.48659260193061843\n",
      "episode :  0\n",
      "current step :  103\n",
      "reward :  -0.47759301307004354\n",
      "episode :  0\n",
      "current step :  104\n",
      "reward :  -0.5164709942936396\n",
      "episode :  0\n",
      "current step :  105\n",
      "reward :  -0.5188893408948178\n",
      "episode :  0\n",
      "current step :  106\n",
      "reward :  -0.4723419100561666\n",
      "episode :  0\n",
      "current step :  107\n",
      "reward :  -0.48707318685753986\n",
      "episode :  0\n",
      "current step :  108\n",
      "reward :  -0.5218491567434206\n",
      "episode :  0\n",
      "current step :  109\n",
      "reward :  -0.5360881570101239\n",
      "episode :  0\n",
      "current step :  110\n",
      "reward :  -0.5284563560258678\n",
      "episode :  0\n",
      "current step :  111\n",
      "reward :  -0.5218400075109617\n",
      "episode :  0\n",
      "current step :  112\n",
      "reward :  -0.5074695426954567\n",
      "episode :  0\n",
      "current step :  113\n",
      "reward :  -0.5473939467965873\n",
      "episode :  0\n",
      "current step :  114\n",
      "reward :  -0.46263388609360817\n",
      "episode :  0\n",
      "current step :  115\n",
      "reward :  -0.46904496211388835\n",
      "episode :  0\n",
      "current step :  116\n",
      "reward :  -0.4660146122957913\n",
      "episode :  0\n",
      "current step :  117\n",
      "reward :  -0.48492649189968007\n",
      "episode :  0\n",
      "current step :  118\n",
      "reward :  -0.28857310734010155\n",
      "episode :  0\n",
      "current step :  119\n",
      "reward :  -0.46822643565924355\n",
      "episode :  0\n",
      "current step :  120\n",
      "reward :  -0.5183358016208638\n",
      "episode :  0\n",
      "current step :  121\n",
      "reward :  -0.3642947478187493\n",
      "episode :  0\n",
      "current step :  122\n",
      "reward :  -0.5116150798730092\n",
      "episode :  0\n",
      "current step :  123\n",
      "reward :  -0.42335099967515954\n",
      "episode :  0\n",
      "current step :  124\n",
      "reward :  -0.503924038557725\n",
      "episode :  0\n",
      "current step :  125\n",
      "reward :  -0.47298631113206485\n",
      "episode :  0\n",
      "current step :  126\n",
      "reward :  -0.3048901995311169\n",
      "episode :  0\n",
      "current step :  127\n",
      "reward :  -0.2887220078728377\n",
      "episode :  0\n",
      "current step :  128\n",
      "reward :  -0.4492758431695992\n",
      "episode :  0\n",
      "current step :  129\n",
      "reward :  -0.44217628708593476\n",
      "episode :  0\n",
      "current step :  130\n",
      "reward :  -0.44390327832139836\n",
      "episode :  0\n",
      "current step :  131\n",
      "reward :  -0.43301649326462294\n",
      "episode :  0\n",
      "current step :  132\n",
      "reward :  -0.3711362606092995\n",
      "episode :  0\n",
      "current step :  133\n",
      "reward :  -0.2872697764645425\n",
      "episode :  0\n",
      "current step :  134\n",
      "reward :  -0.4299240466998613\n",
      "episode :  0\n",
      "current step :  135\n",
      "reward :  -0.28934719821513694\n",
      "episode :  0\n",
      "current step :  136\n",
      "reward :  -0.34422399586791746\n",
      "episode :  0\n",
      "current step :  137\n",
      "reward :  -0.39666463205293107\n",
      "episode :  0\n",
      "current step :  138\n",
      "reward :  -0.4399068650721354\n",
      "episode :  0\n",
      "current step :  139\n",
      "reward :  -0.4758641404186846\n",
      "episode :  0\n",
      "current step :  140\n",
      "reward :  -0.31276154777697923\n",
      "episode :  0\n",
      "current step :  141\n",
      "reward :  -0.3630855432556267\n",
      "episode :  0\n",
      "current step :  142\n",
      "reward :  -0.3807201733955762\n",
      "episode :  0\n",
      "current step :  143\n",
      "reward :  -0.3936943114465896\n",
      "episode :  0\n",
      "current step :  144\n",
      "reward :  -0.4217356009223013\n",
      "episode :  0\n",
      "current step :  145\n",
      "reward :  -0.40706374377492777\n",
      "episode :  0\n",
      "current step :  146\n",
      "reward :  -0.5977168086656321\n",
      "episode :  0\n",
      "current step :  147\n",
      "reward :  -0.48714336857280904\n",
      "episode :  0\n",
      "current step :  148\n",
      "reward :  -0.5774901815682849\n",
      "episode :  0\n",
      "current step :  149\n",
      "reward :  -0.47324144174993227\n",
      "episode :  0\n",
      "current step :  150\n",
      "reward :  -0.42935615212952066\n",
      "episode :  0\n",
      "current step :  151\n",
      "reward :  -0.6383807227383644\n",
      "episode :  0\n",
      "current step :  152\n",
      "reward :  -0.6313571005123255\n",
      "episode :  0\n",
      "current step :  153\n",
      "reward :  -0.5000288957661289\n",
      "episode :  0\n",
      "current step :  154\n",
      "reward :  -0.6119527008857919\n",
      "episode :  0\n",
      "current step :  155\n",
      "reward :  -0.6388438661359094\n",
      "episode :  0\n",
      "current step :  156\n",
      "reward :  -0.6239009490987615\n",
      "episode :  0\n",
      "current step :  157\n",
      "reward :  -0.5584075529702296\n",
      "episode :  0\n",
      "current step :  158\n",
      "reward :  -0.5878209208143435\n",
      "episode :  0\n",
      "current step :  159\n",
      "reward :  -0.679840481127551\n",
      "episode :  0\n",
      "current step :  160\n",
      "reward :  -0.7197204252749518\n",
      "episode :  0\n",
      "current step :  161\n",
      "reward :  -0.7235451372665761\n",
      "episode :  0\n",
      "current step :  162\n",
      "reward :  -0.6352220315217443\n",
      "episode :  0\n",
      "current step :  163\n",
      "reward :  -0.7079219759437327\n",
      "episode :  0\n",
      "current step :  164\n",
      "reward :  -0.5572825578550937\n",
      "episode :  0\n",
      "current step :  165\n",
      "reward :  -0.6455452576061848\n",
      "episode :  0\n",
      "current step :  166\n",
      "reward :  -0.6651646897464633\n",
      "episode :  0\n",
      "current step :  167\n",
      "reward :  -0.639499855430019\n",
      "episode :  0\n",
      "current step :  168\n",
      "reward :  -0.4835118114355517\n",
      "episode :  0\n",
      "current step :  169\n",
      "reward :  -0.6301205178288578\n",
      "episode :  0\n",
      "current step :  170\n",
      "reward :  -0.5820108081024048\n",
      "episode :  0\n",
      "current step :  171\n",
      "reward :  -0.7191285540002943\n",
      "episode :  0\n",
      "current step :  172\n",
      "reward :  -0.650026507253987\n",
      "episode :  0\n",
      "current step :  173\n",
      "reward :  -0.5642000931935368\n",
      "episode :  0\n",
      "current step :  174\n",
      "reward :  -0.7168228602396026\n",
      "episode :  0\n",
      "current step :  175\n",
      "reward :  -0.5838746013302575\n",
      "episode :  0\n",
      "current step :  176\n",
      "reward :  -0.6749336141387441\n",
      "episode :  0\n",
      "current step :  177\n",
      "reward :  -0.5765987446473559\n",
      "episode :  0\n",
      "current step :  178\n",
      "reward :  -0.5956867930398436\n",
      "episode :  0\n",
      "current step :  179\n",
      "reward :  -0.6979439078256968\n",
      "episode :  0\n",
      "current step :  180\n",
      "reward :  -0.6832136387343034\n",
      "episode :  0\n",
      "current step :  181\n",
      "reward :  -0.538886781709544\n",
      "episode :  0\n",
      "current step :  182\n",
      "reward :  -0.6495644517578965\n",
      "episode :  0\n",
      "current step :  183\n",
      "reward :  -0.7173148622880419\n",
      "episode :  0\n",
      "current step :  184\n",
      "reward :  -0.4801208598621391\n",
      "episode :  0\n",
      "current step :  185\n",
      "reward :  -0.7028470683638722\n",
      "episode :  0\n",
      "current step :  186\n",
      "reward :  -0.6667679541945325\n",
      "episode :  0\n",
      "current step :  187\n",
      "reward :  -0.6920397063855015\n",
      "episode :  0\n",
      "current step :  188\n",
      "reward :  -0.50628358461225\n",
      "episode :  0\n",
      "current step :  189\n",
      "reward :  -0.2949350413496363\n",
      "episode :  0\n",
      "current step :  190\n",
      "reward :  -0.7073469008568085\n",
      "episode :  0\n",
      "current step :  191\n",
      "reward :  -0.36762719186336407\n",
      "episode :  0\n",
      "current step :  192\n",
      "reward :  -0.37056217986312606\n",
      "episode :  0\n",
      "current step :  193\n",
      "reward :  -0.3638538226666674\n",
      "episode :  0\n",
      "current step :  194\n",
      "reward :  -0.6505456494196878\n",
      "episode :  0\n",
      "current step :  195\n",
      "reward :  -0.5171481439952345\n",
      "episode :  0\n",
      "current step :  196\n",
      "reward :  -0.6019984311916283\n",
      "episode :  0\n",
      "current step :  197\n",
      "reward :  -0.5221760278883253\n",
      "episode :  0\n",
      "current step :  198\n",
      "reward :  -0.55937022845669\n",
      "episode :  0\n",
      "current step :  199\n",
      "reward :  -0.7213688955132608\n",
      "episode :  0\n",
      "current step :  200\n",
      "reward :  -0.2715254530894891\n",
      "episode :  0\n",
      "current step :  201\n",
      "reward :  -0.5219722272326848\n",
      "episode :  0\n",
      "current step :  202\n",
      "reward :  -0.3161344016429583\n",
      "episode :  0\n",
      "current step :  203\n",
      "reward :  -0.6214217586194785\n",
      "episode :  0\n",
      "current step :  204\n",
      "reward :  -0.5102150358045544\n",
      "episode :  0\n",
      "current step :  205\n",
      "reward :  -0.4510236593732851\n",
      "episode :  0\n",
      "current step :  206\n",
      "reward :  -0.5213709016444331\n",
      "episode :  0\n",
      "current step :  207\n",
      "reward :  -0.597397942462339\n",
      "episode :  0\n",
      "current step :  208\n",
      "reward :  -0.6576646560556522\n",
      "episode :  0\n",
      "current step :  209\n",
      "reward :  -0.5992380836964744\n",
      "episode :  0\n",
      "current step :  210\n",
      "reward :  -0.4650601337539216\n",
      "episode :  0\n",
      "current step :  211\n",
      "reward :  -0.5637366260386804\n",
      "episode :  0\n",
      "current step :  212\n",
      "reward :  -0.7023900987681605\n",
      "episode :  0\n",
      "current step :  213\n",
      "reward :  -0.42900735682385394\n",
      "episode :  0\n",
      "current step :  214\n",
      "reward :  -0.6173760329937988\n",
      "episode :  0\n",
      "current step :  215\n",
      "reward :  -0.4932520614679394\n",
      "episode :  0\n",
      "current step :  216\n",
      "reward :  -0.34404290666886306\n",
      "episode :  0\n",
      "current step :  217\n",
      "reward :  -0.46185498219346716\n",
      "episode :  0\n",
      "current step :  218\n",
      "reward :  -0.6184266471020673\n",
      "episode :  0\n",
      "current step :  219\n",
      "reward :  -0.4915870848611518\n",
      "episode :  0\n",
      "current step :  220\n",
      "reward :  -0.4737223838424063\n",
      "episode :  0\n",
      "current step :  221\n",
      "reward :  -0.4090612700274678\n",
      "episode :  0\n",
      "current step :  222\n",
      "reward :  -0.6420233802943612\n",
      "episode :  0\n",
      "current step :  223\n",
      "reward :  -0.39485858404258867\n",
      "episode :  0\n",
      "current step :  224\n",
      "reward :  -0.5285150865735043\n",
      "episode :  0\n",
      "current step :  225\n",
      "reward :  -0.6492803417110092\n",
      "episode :  0\n",
      "current step :  226\n",
      "reward :  -0.510400298433855\n",
      "episode :  0\n",
      "current step :  227\n",
      "reward :  -0.5460529008148431\n",
      "episode :  0\n",
      "current step :  228\n",
      "reward :  -0.35653667858186633\n",
      "episode :  0\n",
      "current step :  229\n",
      "reward :  -0.5006253892866059\n",
      "episode :  0\n",
      "current step :  230\n",
      "reward :  -0.3346228925729878\n",
      "episode :  0\n",
      "current step :  231\n",
      "reward :  -0.5768776384420369\n",
      "episode :  0\n",
      "current step :  232\n",
      "reward :  -0.4983579675845733\n",
      "episode :  0\n",
      "current step :  233\n",
      "reward :  -0.38831433251358644\n",
      "episode :  0\n",
      "current step :  234\n",
      "reward :  -0.6071363026595152\n",
      "episode :  0\n",
      "current step :  235\n",
      "reward :  -0.4931601656413526\n",
      "episode :  0\n",
      "current step :  236\n",
      "reward :  -0.3571503214918117\n",
      "episode :  0\n",
      "current step :  237\n",
      "reward :  -0.45273953769684655\n",
      "episode :  0\n",
      "current step :  238\n",
      "reward :  -0.5643244321922717\n",
      "episode :  0\n",
      "current step :  239\n",
      "reward :  -0.443506065493028\n",
      "episode :  0\n",
      "current step :  240\n",
      "reward :  -0.3582472278423378\n",
      "episode :  0\n",
      "current step :  241\n",
      "reward :  -0.376011843462684\n",
      "episode :  0\n",
      "current step :  242\n",
      "reward :  -0.5241830106163308\n",
      "episode :  0\n",
      "current step :  243\n",
      "reward :  -0.4017966835752771\n",
      "episode :  0\n",
      "current step :  244\n",
      "reward :  -0.3686202629376943\n",
      "episode :  0\n",
      "current step :  245\n",
      "reward :  -0.567233881129849\n",
      "episode :  0\n",
      "current step :  246\n",
      "reward :  -0.39788991102964705\n",
      "episode :  0\n",
      "current step :  247\n",
      "reward :  -0.3339800697757566\n",
      "episode :  0\n",
      "current step :  248\n",
      "reward :  -0.24391681243021626\n",
      "episode :  0\n",
      "current step :  249\n",
      "reward :  -0.28092378145282726\n",
      "episode :  0\n",
      "current step :  250\n",
      "reward :  -0.2498685528945136\n",
      "episode :  0\n",
      "current step :  251\n",
      "reward :  -0.5436748380154091\n",
      "episode :  0\n",
      "current step :  252\n",
      "reward :  -0.5999425936693892\n",
      "episode :  0\n",
      "current step :  253\n",
      "reward :  -0.5275970519368242\n",
      "episode :  0\n",
      "current step :  254\n",
      "reward :  -0.41420296413526586\n",
      "episode :  0\n",
      "current step :  255\n",
      "reward :  -0.3248929416918078\n",
      "episode :  0\n",
      "current step :  256\n",
      "reward :  -0.38853405671895797\n",
      "episode :  0\n",
      "current step :  257\n",
      "reward :  -0.39942878901906037\n",
      "episode :  0\n",
      "current step :  258\n",
      "reward :  -0.48158952458598514\n",
      "episode :  0\n",
      "current step :  259\n",
      "reward :  -0.3029144993313521\n",
      "episode :  0\n",
      "current step :  260\n",
      "reward :  -0.48073041323654864\n",
      "episode :  0\n",
      "current step :  261\n",
      "reward :  -0.5057489352155099\n",
      "episode :  0\n",
      "current step :  262\n",
      "reward :  -0.5064031862303839\n",
      "episode :  0\n",
      "current step :  263\n",
      "reward :  -0.3213917655853374\n",
      "episode :  0\n",
      "current step :  264\n",
      "reward :  -0.5884411951090043\n",
      "episode :  0\n",
      "current step :  265\n",
      "reward :  -0.4498456727179671\n",
      "episode :  0\n",
      "current step :  266\n",
      "reward :  -0.38044533159292454\n",
      "episode :  0\n",
      "current step :  267\n",
      "reward :  -0.5805737285968289\n",
      "episode :  0\n",
      "current step :  268\n",
      "reward :  -0.5170771655687817\n",
      "episode :  0\n",
      "current step :  269\n",
      "reward :  -0.4690557000459335\n",
      "episode :  0\n",
      "current step :  270\n",
      "reward :  -0.3478247264155363\n",
      "episode :  0\n",
      "current step :  271\n",
      "reward :  -0.3059057933887434\n",
      "episode :  0\n",
      "current step :  272\n",
      "reward :  -0.46683166281286126\n",
      "episode :  0\n",
      "current step :  273\n",
      "reward :  -0.2835445155943152\n",
      "episode :  0\n",
      "current step :  274\n",
      "reward :  -0.302626167568302\n",
      "episode :  0\n",
      "current step :  275\n",
      "reward :  -0.5321700078407661\n",
      "episode :  0\n",
      "current step :  276\n",
      "reward :  -0.37081010548739635\n",
      "episode :  0\n",
      "current step :  277\n",
      "reward :  -0.38094778175293936\n",
      "episode :  0\n",
      "current step :  278\n",
      "reward :  -0.439868450212021\n",
      "episode :  0\n",
      "current step :  279\n",
      "reward :  -0.5619762310229753\n",
      "episode :  0\n",
      "current step :  280\n",
      "reward :  -0.5088296809862832\n",
      "episode :  0\n",
      "current step :  281\n",
      "reward :  -0.5366915683249167\n",
      "episode :  0\n",
      "current step :  282\n",
      "reward :  -0.4113730477603264\n",
      "episode :  0\n",
      "current step :  283\n",
      "reward :  -0.27090746629810364\n",
      "episode :  0\n",
      "current step :  284\n",
      "reward :  -0.3374112506373213\n",
      "episode :  0\n",
      "current step :  285\n",
      "reward :  -0.483339238413473\n",
      "episode :  1\n"
     ]
    }
   ],
   "source": [
    "#del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = SAC.load(\"sac_imitation\")\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = env.step(action.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404953b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
