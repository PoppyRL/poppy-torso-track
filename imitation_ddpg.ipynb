{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c47c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "\n",
    "import sys\n",
    "sys.path.append('E:\\Anaconda\\envs\\gym-examples')\n",
    "import gym_examples\n",
    "from gym.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238796fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello, I am Poppy!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.10876056, -0.17632997,  0.07149935, -0.10220377, -0.18020962,\n",
       "        0.07149935], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('gym_examples/Poppy-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f61b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "model.learn(total_timesteps=100, log_interval=10)\n",
    "model.save(\"ddpg_imitation\")\n",
    "vec_env = model.get_env()\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DDPG.load(\"ddpg_imitation\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba77d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "\n",
    "model = DDPG.load(\"ddpg_imitation\")\n",
    "model.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d3088d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step :  0\n",
      "reward :  -0.4454171387169538\n",
      "current step :  1\n",
      "reward :  -0.44473869396998483\n",
      "current step :  2\n",
      "reward :  -0.4451434100122294\n",
      "current step :  3\n",
      "reward :  -0.44568206695897744\n",
      "current step :  4\n",
      "reward :  -0.4450343231725972\n",
      "current step :  5\n",
      "reward :  -0.44417112357213995\n",
      "current step :  6\n",
      "reward :  -0.443009084795555\n",
      "current step :  7\n",
      "reward :  -0.4415157493820922\n",
      "current step :  8\n",
      "reward :  -0.4398868812963723\n",
      "current step :  9\n",
      "reward :  -0.4357594938479735\n",
      "current step :  10\n",
      "reward :  -0.43171974228216603\n",
      "current step :  11\n",
      "reward :  -0.42777305854163317\n",
      "current step :  12\n",
      "reward :  -0.42311643325188036\n",
      "current step :  13\n",
      "reward :  -0.4186179169447009\n",
      "current step :  14\n",
      "reward :  -0.41442517075468244\n",
      "current step :  15\n",
      "reward :  -0.41019153841559336\n",
      "current step :  16\n",
      "reward :  -0.40547718810164\n",
      "current step :  17\n",
      "reward :  -0.4008322441878091\n",
      "current step :  18\n",
      "reward :  -0.3970783469804646\n",
      "current step :  19\n",
      "reward :  -0.3946795807726316\n",
      "current step :  20\n",
      "reward :  -0.39305869829644946\n",
      "current step :  21\n",
      "reward :  -0.392565104862217\n",
      "current step :  22\n",
      "reward :  -0.39337704734621604\n",
      "current step :  23\n",
      "reward :  -0.394978204325397\n",
      "current step :  24\n",
      "reward :  -0.39811545739180754\n",
      "current step :  25\n",
      "reward :  -0.40199686969180165\n",
      "current step :  26\n",
      "reward :  -0.40660515715313367\n",
      "current step :  27\n",
      "reward :  -0.4115744575232123\n",
      "current step :  28\n",
      "reward :  -0.4172214928071351\n",
      "current step :  29\n",
      "reward :  -0.41947934947630844\n",
      "current step :  30\n",
      "reward :  -0.42512554486473264\n",
      "current step :  31\n",
      "reward :  -0.431941356065056\n",
      "current step :  32\n",
      "reward :  -0.4389119717058935\n",
      "current step :  33\n",
      "reward :  -0.4450171734570169\n",
      "current step :  34\n",
      "reward :  -0.4515377382028932\n",
      "current step :  35\n",
      "reward :  -0.45709750233109697\n",
      "current step :  36\n",
      "reward :  -0.4583173155464549\n",
      "current step :  37\n",
      "reward :  -0.458372555943627\n",
      "current step :  38\n",
      "reward :  -0.4578840610645379\n",
      "current step :  39\n",
      "reward :  -0.4555365900577197\n",
      "current step :  40\n",
      "reward :  -0.45291050353943235\n",
      "current step :  41\n",
      "reward :  -0.45099843493768316\n",
      "current step :  42\n",
      "reward :  -0.45044186310235734\n",
      "current step :  43\n",
      "reward :  -0.450447174728894\n",
      "current step :  44\n",
      "reward :  -0.45193134871590446\n",
      "current step :  45\n",
      "reward :  -0.4530003261805964\n",
      "current step :  46\n",
      "reward :  -0.45507770803841885\n",
      "current step :  47\n",
      "reward :  -0.456469629477474\n",
      "current step :  48\n",
      "reward :  -0.45867829083412137\n",
      "current step :  49\n",
      "reward :  -0.46130275766964296\n",
      "current step :  50\n",
      "reward :  -0.4638265753773431\n",
      "current step :  51\n",
      "reward :  -0.4658702261836384\n",
      "current step :  52\n",
      "reward :  -0.46829728510735236\n",
      "current step :  53\n",
      "reward :  -0.47023028744874873\n",
      "current step :  54\n",
      "reward :  -0.4720660188602466\n",
      "current step :  55\n",
      "reward :  -0.47397635730889087\n",
      "current step :  56\n",
      "reward :  -0.47569491910998807\n",
      "current step :  57\n",
      "reward :  -0.47687847546839873\n",
      "current step :  58\n",
      "reward :  -0.47780706572580817\n",
      "current step :  59\n",
      "reward :  -0.4786309140287271\n",
      "current step :  60\n",
      "reward :  -0.47909053335207885\n",
      "current step :  61\n",
      "reward :  -0.4797099014310625\n",
      "current step :  62\n",
      "reward :  -0.48013765305503014\n",
      "current step :  63\n",
      "reward :  -0.4803981386514985\n",
      "current step :  64\n",
      "reward :  -0.48032102521481057\n",
      "current step :  65\n",
      "reward :  -0.48005896760403327\n",
      "current step :  66\n",
      "reward :  -0.47946632601119904\n",
      "current step :  67\n",
      "reward :  -0.47844821547405214\n",
      "current step :  68\n",
      "reward :  -0.47694965163605907\n",
      "current step :  69\n",
      "reward :  -0.4752583945083978\n",
      "current step :  70\n",
      "reward :  -0.4734675644836048\n",
      "current step :  71\n",
      "reward :  -0.4714346565955108\n",
      "current step :  72\n",
      "reward :  -0.46916425465687944\n",
      "current step :  73\n",
      "reward :  -0.4665223665279408\n",
      "current step :  74\n",
      "reward :  -0.4628546545349979\n",
      "current step :  75\n",
      "reward :  -0.45931382379118024\n",
      "current step :  76\n",
      "reward :  -0.455641327713057\n",
      "current step :  77\n",
      "reward :  -0.45165928563128843\n",
      "current step :  78\n",
      "reward :  -0.44724515672161524\n",
      "current step :  79\n",
      "reward :  -0.44282795257652524\n",
      "current step :  80\n",
      "reward :  -0.43819649238785513\n",
      "current step :  81\n",
      "reward :  -0.43104209260686704\n",
      "current step :  82\n",
      "reward :  -0.42475127595012946\n",
      "current step :  83\n",
      "reward :  -0.4190878001260696\n",
      "current step :  84\n",
      "reward :  -0.4137166938909999\n",
      "current step :  85\n",
      "reward :  -0.4089700072632059\n",
      "current step :  86\n",
      "reward :  -0.40401217717025806\n",
      "current step :  87\n",
      "reward :  -0.39951534231780766\n",
      "current step :  88\n",
      "reward :  -0.39580572442269124\n",
      "current step :  89\n",
      "reward :  -0.39266641905626815\n",
      "current step :  90\n",
      "reward :  -0.39060560583079884\n",
      "current step :  91\n",
      "reward :  -0.38906435708442483\n",
      "current step :  92\n",
      "reward :  -0.3873647099818695\n",
      "current step :  93\n",
      "reward :  -0.38646575629316027\n",
      "current step :  94\n",
      "reward :  -0.38717661652935625\n",
      "current step :  95\n",
      "reward :  -0.38809732985140744\n",
      "current step :  96\n",
      "reward :  -0.3878828304440265\n",
      "current step :  97\n",
      "reward :  -0.38871231133239137\n",
      "current step :  98\n",
      "reward :  -0.3900077402211486\n",
      "current step :  99\n",
      "reward :  -0.3931217176335578\n",
      "current step :  100\n",
      "reward :  -0.3962461343748873\n",
      "current step :  101\n",
      "reward :  -0.40074879550425074\n",
      "current step :  102\n",
      "reward :  -0.4049073041909543\n",
      "current step :  103\n",
      "reward :  -0.4097999038643325\n",
      "current step :  104\n",
      "reward :  -0.41516800630576517\n",
      "current step :  105\n",
      "reward :  -0.4206713852305818\n",
      "current step :  106\n",
      "reward :  -0.4259997525534977\n",
      "current step :  107\n",
      "reward :  -0.43113162783304687\n",
      "current step :  108\n",
      "reward :  -0.4355603385989919\n",
      "current step :  109\n",
      "reward :  -0.4398465415491703\n",
      "current step :  110\n",
      "reward :  -0.4434966758443211\n",
      "current step :  111\n",
      "reward :  -0.4468514207439612\n",
      "current step :  112\n",
      "reward :  -0.45034286738182605\n",
      "current step :  113\n",
      "reward :  -0.45367077359287594\n",
      "current step :  114\n",
      "reward :  -0.45596847385189154\n",
      "current step :  115\n",
      "reward :  -0.458040720740653\n",
      "current step :  116\n",
      "reward :  -0.4600500332806903\n",
      "current step :  117\n",
      "reward :  -0.4612461466693504\n",
      "current step :  118\n",
      "reward :  -0.4618598380825156\n",
      "current step :  119\n",
      "reward :  -0.4622690577425201\n",
      "current step :  120\n",
      "reward :  -0.4621193540481211\n",
      "current step :  121\n",
      "reward :  -0.46190278289207265\n",
      "current step :  122\n",
      "reward :  -0.4614555070488111\n",
      "current step :  123\n",
      "reward :  -0.4604777595094053\n",
      "current step :  124\n",
      "reward :  -0.4592891336662918\n",
      "current step :  125\n",
      "reward :  -0.4578320308669547\n",
      "current step :  126\n",
      "reward :  -0.45589171004128165\n",
      "current step :  127\n",
      "reward :  -0.45370901120138235\n",
      "current step :  128\n",
      "reward :  -0.4518168252663328\n",
      "current step :  129\n",
      "reward :  -0.4496765032248508\n",
      "current step :  130\n",
      "reward :  -0.44726284366628444\n",
      "current step :  131\n",
      "reward :  -0.44489956604119457\n",
      "current step :  132\n",
      "reward :  -0.44214660717316717\n",
      "current step :  133\n",
      "reward :  -0.4391060073003851\n",
      "current step :  134\n",
      "reward :  -0.43552385305773017\n",
      "current step :  135\n",
      "reward :  -0.43208545901349843\n",
      "current step :  136\n",
      "reward :  -0.42851674464394085\n",
      "current step :  137\n",
      "reward :  -0.4249600571956536\n",
      "current step :  138\n",
      "reward :  -0.42132696389059604\n",
      "current step :  139\n",
      "reward :  -0.41729372045637486\n",
      "current step :  140\n",
      "reward :  -0.4145700076535999\n",
      "current step :  141\n",
      "reward :  -0.4125550655676984\n",
      "current step :  142\n",
      "reward :  -0.41061613951899095\n",
      "current step :  143\n",
      "reward :  -0.4091961794253566\n",
      "current step :  144\n",
      "reward :  -0.4089636776680446\n",
      "current step :  145\n",
      "reward :  -0.41095506269051785\n",
      "current step :  146\n",
      "reward :  -0.41385211092234203\n",
      "current step :  147\n",
      "reward :  -0.4204261405127319\n",
      "current step :  148\n",
      "reward :  -0.42616963015052584\n",
      "current step :  149\n",
      "reward :  -0.4309229453993507\n",
      "current step :  150\n",
      "reward :  -0.4392606121049698\n",
      "current step :  151\n",
      "reward :  -0.44785312404567246\n",
      "current step :  152\n",
      "reward :  -0.4575659168258824\n",
      "current step :  153\n",
      "reward :  -0.4658326184450976\n",
      "current step :  154\n",
      "reward :  -0.47690512264320667\n",
      "current step :  155\n",
      "reward :  -0.48849679791402567\n",
      "current step :  156\n",
      "reward :  -0.4995430672598459\n",
      "current step :  157\n",
      "reward :  -0.5108837126511687\n",
      "current step :  158\n",
      "reward :  -0.519982954128936\n",
      "current step :  159\n",
      "reward :  -0.5262828235516037\n",
      "current step :  160\n",
      "reward :  -0.5292262620247825\n",
      "current step :  161\n",
      "reward :  -0.5310281845874439\n",
      "current step :  162\n",
      "reward :  -0.5320378312465117\n",
      "current step :  163\n",
      "reward :  -0.5353865459488754\n",
      "current step :  164\n",
      "reward :  -0.5401276488187572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step :  165\n",
      "reward :  -0.5440039658113286\n",
      "current step :  166\n",
      "reward :  -0.5477939260501248\n",
      "current step :  167\n",
      "reward :  -0.550086575382025\n",
      "current step :  168\n",
      "reward :  -0.5533433223663353\n",
      "current step :  169\n",
      "reward :  -0.5549007572283674\n",
      "current step :  170\n",
      "reward :  -0.5548297178454745\n",
      "current step :  171\n",
      "reward :  -0.5545580126864844\n",
      "current step :  172\n",
      "reward :  -0.5554400523560253\n",
      "current step :  173\n",
      "reward :  -0.5571349463121086\n",
      "current step :  174\n",
      "reward :  -0.5587942653122905\n",
      "current step :  175\n",
      "reward :  -0.560640217040356\n",
      "current step :  176\n",
      "reward :  -0.5628135213868934\n",
      "current step :  177\n",
      "reward :  -0.5640167254938844\n",
      "current step :  178\n",
      "reward :  -0.5648649753926208\n",
      "current step :  179\n",
      "reward :  -0.5653777656268492\n",
      "current step :  180\n",
      "reward :  -0.5655619687397987\n",
      "current step :  181\n",
      "reward :  -0.5656809780988776\n",
      "current step :  182\n",
      "reward :  -0.5661595429426984\n",
      "current step :  183\n",
      "reward :  -0.5664330871624297\n",
      "current step :  184\n",
      "reward :  -0.5669773953739655\n",
      "current step :  185\n",
      "reward :  -0.5680268886102984\n",
      "current step :  186\n",
      "reward :  -0.5689755851407345\n",
      "current step :  187\n",
      "reward :  -0.5692339018622518\n",
      "current step :  188\n",
      "reward :  -0.5693488182994423\n",
      "current step :  189\n",
      "reward :  -0.5697440089425786\n",
      "current step :  190\n",
      "reward :  -0.5703843706657299\n",
      "current step :  191\n",
      "reward :  -0.570780536385735\n",
      "current step :  192\n",
      "reward :  -0.5712805446199887\n",
      "current step :  193\n",
      "reward :  -0.5721209090752991\n",
      "current step :  194\n",
      "reward :  -0.572486825938754\n",
      "current step :  195\n",
      "reward :  -0.5728961168574288\n",
      "current step :  196\n",
      "reward :  -0.5731071957864332\n",
      "current step :  197\n",
      "reward :  -0.5729306571104233\n",
      "current step :  198\n",
      "reward :  -0.5729052291814251\n",
      "current step :  199\n",
      "reward :  -0.5727452000183538\n",
      "current step :  200\n",
      "reward :  -0.5726702765148166\n",
      "current step :  201\n",
      "reward :  -0.5723151931438438\n",
      "current step :  202\n",
      "reward :  -0.5720379961843323\n",
      "current step :  203\n",
      "reward :  -0.5714302170525452\n",
      "current step :  204\n",
      "reward :  -0.5702310249813849\n",
      "current step :  205\n",
      "reward :  -0.5688350373881961\n",
      "current step :  206\n",
      "reward :  -0.5672605455258967\n",
      "current step :  207\n",
      "reward :  -0.5659329331436381\n",
      "current step :  208\n",
      "reward :  -0.5637840780159828\n",
      "current step :  209\n",
      "reward :  -0.5614743685318377\n",
      "current step :  210\n",
      "reward :  -0.5588404264726977\n",
      "current step :  211\n",
      "reward :  -0.55651387778761\n",
      "current step :  212\n",
      "reward :  -0.5537342816418682\n",
      "current step :  213\n",
      "reward :  -0.5511324797310939\n",
      "current step :  214\n",
      "reward :  -0.5490855805694406\n",
      "current step :  215\n",
      "reward :  -0.5468964822372513\n",
      "current step :  216\n",
      "reward :  -0.5453214964830501\n",
      "current step :  217\n",
      "reward :  -0.5443707713959581\n",
      "current step :  218\n",
      "reward :  -0.5437363448070273\n",
      "current step :  219\n",
      "reward :  -0.5433421703640674\n",
      "current step :  220\n",
      "reward :  -0.542607473765816\n",
      "current step :  221\n",
      "reward :  -0.5417956308340285\n",
      "current step :  222\n",
      "reward :  -0.5392146884950716\n",
      "current step :  223\n",
      "reward :  -0.5369134923929957\n",
      "current step :  224\n",
      "reward :  -0.5347852254020176\n",
      "current step :  225\n",
      "reward :  -0.5326005112053956\n",
      "current step :  226\n",
      "reward :  -0.5316793248445311\n",
      "current step :  227\n",
      "reward :  -0.5314379588384751\n",
      "current step :  228\n",
      "reward :  -0.5304399424145585\n",
      "current step :  229\n",
      "reward :  -0.5270368526511561\n",
      "current step :  230\n",
      "reward :  -0.5231396812297262\n",
      "current step :  231\n",
      "reward :  -0.5177808423729515\n",
      "current step :  232\n",
      "reward :  -0.5125944618411156\n",
      "current step :  233\n",
      "reward :  -0.5053076122980529\n",
      "current step :  234\n",
      "reward :  -0.49733675023731294\n",
      "current step :  235\n",
      "reward :  -0.48954244902395994\n",
      "current step :  236\n",
      "reward :  -0.4803963300503082\n",
      "current step :  237\n",
      "reward :  -0.47290538923773484\n",
      "current step :  238\n",
      "reward :  -0.46646066104598016\n",
      "current step :  239\n",
      "reward :  -0.46085134934148064\n",
      "current step :  240\n",
      "reward :  -0.4555183937496932\n",
      "current step :  241\n",
      "reward :  -0.4495991736838924\n",
      "current step :  242\n",
      "reward :  -0.4444965260224439\n",
      "current step :  243\n",
      "reward :  -0.440573927953925\n",
      "current step :  244\n",
      "reward :  -0.4381194743082069\n",
      "current step :  245\n",
      "reward :  -0.43615556092140584\n",
      "current step :  246\n",
      "reward :  -0.41775403195477834\n",
      "current step :  247\n",
      "reward :  -0.4033314694098345\n",
      "current step :  248\n",
      "reward :  -0.3994310292323296\n",
      "current step :  249\n",
      "reward :  -0.3988260691184877\n",
      "current step :  250\n",
      "reward :  -0.39915659204880627\n",
      "current step :  251\n",
      "reward :  -0.40120899651926867\n",
      "current step :  252\n",
      "reward :  -0.4039563132175281\n",
      "current step :  253\n",
      "reward :  -0.4065532087509623\n",
      "current step :  254\n",
      "reward :  -0.40942628868525616\n",
      "current step :  255\n",
      "reward :  -0.4121748208662454\n",
      "current step :  256\n",
      "reward :  -0.4149317538987612\n",
      "current step :  257\n",
      "reward :  -0.4173409660206085\n",
      "current step :  258\n",
      "reward :  -0.4202190908302537\n",
      "current step :  259\n",
      "reward :  -0.4230350844719934\n",
      "current step :  260\n",
      "reward :  -0.42563931972562075\n",
      "current step :  261\n",
      "reward :  -0.4285476837133035\n",
      "current step :  262\n",
      "reward :  -0.43125340503955295\n",
      "current step :  263\n",
      "reward :  -0.43378354329148106\n",
      "current step :  264\n",
      "reward :  -0.4356969341390734\n",
      "current step :  265\n",
      "reward :  -0.4375610451674708\n",
      "current step :  266\n",
      "reward :  -0.43884317395409106\n",
      "current step :  267\n",
      "reward :  -0.4402005919284011\n",
      "current step :  268\n",
      "reward :  -0.4415372839323239\n",
      "current step :  269\n",
      "reward :  -0.4427984833547473\n",
      "current step :  270\n",
      "reward :  -0.4439487540313849\n",
      "current step :  271\n",
      "reward :  -0.4451928497926772\n",
      "current step :  272\n",
      "reward :  -0.4469339581594562\n",
      "current step :  273\n",
      "reward :  -0.44802537031007766\n",
      "current step :  274\n",
      "reward :  -0.4491304818491732\n",
      "current step :  275\n",
      "reward :  -0.4498551247346033\n",
      "current step :  276\n",
      "reward :  -0.45043979871928147\n",
      "current step :  277\n",
      "reward :  -0.45066843481582414\n",
      "current step :  278\n",
      "reward :  -0.450824908880628\n",
      "current step :  279\n",
      "reward :  -0.45082307416758083\n",
      "current step :  280\n",
      "reward :  -0.45091005669491446\n",
      "current step :  281\n",
      "reward :  -0.45102702022855923\n",
      "current step :  282\n",
      "reward :  -0.4511537595019865\n",
      "current step :  283\n",
      "reward :  -0.451221579757166\n",
      "current step :  284\n",
      "reward :  -0.4511173766917955\n",
      "current step :  285\n",
      "reward :  -0.451132892254783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = env.step(action.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1271b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-166.87225  ,  -50.67862  , -155.58098  ,   74.15871  ,\n",
       "         57.36116  ,  143.42847  ,   -3.2159424, -122.096664 ,\n",
       "         51.43799  ,   28.526123 ,  153.29846  ,  -71.49954  ,\n",
       "        -80.46852  ,  -32.0878   ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314bfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
