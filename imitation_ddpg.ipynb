{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c47c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from utils.skeleton import *\n",
    "from utils.quaternion import *\n",
    "from utils.blazepose import blazepose_skeletons\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "\n",
    "import sys\n",
    "sys.path.append('E:\\Anaconda\\envs\\gym-examples')\n",
    "import gym_examples\n",
    "from gym.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238796fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Hello, I am Poppy!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1087558 , -0.17607144,  0.07120024, -0.10220377, -0.18020962,\n",
       "        0.07149935], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('gym_examples/Poppy-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8c2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f61b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "current step :  0\n",
      "reward :  -0.45739814487807495\n",
      "episode :  0\n",
      "current step :  1\n",
      "reward :  -0.3705151361760913\n",
      "episode :  0\n",
      "current step :  2\n",
      "reward :  -0.5428800189170493\n",
      "episode :  0\n",
      "current step :  3\n",
      "reward :  -0.48326996615780254\n",
      "episode :  0\n",
      "current step :  4\n",
      "reward :  -0.5136419496943628\n",
      "episode :  0\n",
      "current step :  5\n",
      "reward :  -0.48219605538799826\n",
      "episode :  0\n",
      "current step :  6\n",
      "reward :  -0.4837612050813444\n",
      "episode :  0\n",
      "current step :  7\n",
      "reward :  -0.5212387739713105\n",
      "episode :  0\n",
      "current step :  8\n",
      "reward :  -0.5040817267809566\n",
      "episode :  0\n",
      "current step :  9\n",
      "reward :  -0.5274291257446315\n",
      "episode :  0\n",
      "current step :  10\n",
      "reward :  -0.6322599748836248\n",
      "episode :  0\n",
      "current step :  11\n",
      "reward :  -0.6009901149209714\n",
      "episode :  0\n",
      "current step :  12\n",
      "reward :  -0.6089002628011264\n",
      "episode :  0\n",
      "current step :  13\n",
      "reward :  -0.6673896614735259\n",
      "episode :  0\n",
      "current step :  14\n",
      "reward :  -0.3783929341429665\n",
      "episode :  0\n",
      "current step :  15\n",
      "reward :  -0.4142025468097263\n",
      "episode :  0\n",
      "current step :  16\n",
      "reward :  -0.4370146677458707\n",
      "episode :  0\n",
      "current step :  17\n",
      "reward :  -0.5519637131881714\n",
      "episode :  0\n",
      "current step :  18\n",
      "reward :  -0.42434632826227237\n",
      "episode :  0\n",
      "current step :  19\n",
      "reward :  -0.3579434006363866\n",
      "episode :  0\n",
      "current step :  20\n",
      "reward :  -0.4807548026057528\n",
      "episode :  0\n",
      "current step :  21\n",
      "reward :  -0.5646304240571307\n",
      "episode :  0\n",
      "current step :  22\n",
      "reward :  -0.539690011660014\n",
      "episode :  0\n",
      "current step :  23\n",
      "reward :  -0.37857326613118947\n",
      "episode :  0\n",
      "current step :  24\n",
      "reward :  -0.5162349549681509\n",
      "episode :  0\n",
      "current step :  25\n",
      "reward :  -0.4247766005112154\n",
      "episode :  0\n",
      "current step :  26\n",
      "reward :  -0.40922843023593775\n",
      "episode :  0\n",
      "current step :  27\n",
      "reward :  -0.35465000204249264\n",
      "episode :  0\n",
      "current step :  28\n",
      "reward :  -0.390518793812733\n",
      "episode :  0\n",
      "current step :  29\n",
      "reward :  -0.39798257666479303\n",
      "episode :  0\n",
      "current step :  30\n",
      "reward :  -0.27282238534758513\n",
      "episode :  0\n",
      "current step :  31\n",
      "reward :  -0.3162097283726364\n",
      "episode :  0\n",
      "current step :  32\n",
      "reward :  -0.40785822849284187\n",
      "episode :  0\n",
      "current step :  33\n",
      "reward :  -0.2690735646774926\n",
      "episode :  0\n",
      "current step :  34\n",
      "reward :  -0.5116829015294467\n",
      "episode :  0\n",
      "current step :  35\n",
      "reward :  -0.4926657586614549\n",
      "episode :  0\n",
      "current step :  36\n",
      "reward :  -0.4721229226880178\n",
      "episode :  0\n",
      "current step :  37\n",
      "reward :  -0.468661111409503\n",
      "episode :  0\n",
      "current step :  38\n",
      "reward :  -0.37134213474949407\n",
      "episode :  0\n",
      "current step :  39\n",
      "reward :  -0.3670982458582034\n",
      "episode :  0\n",
      "current step :  40\n",
      "reward :  -0.4126904200232447\n",
      "episode :  0\n",
      "current step :  41\n",
      "reward :  -0.5026310332451588\n",
      "episode :  0\n",
      "current step :  42\n",
      "reward :  -0.5003071618623456\n",
      "episode :  0\n",
      "current step :  43\n",
      "reward :  -0.4068800787379806\n",
      "episode :  0\n",
      "current step :  44\n",
      "reward :  -0.3419721055110555\n",
      "episode :  0\n",
      "current step :  45\n",
      "reward :  -0.4545610943704782\n",
      "episode :  0\n",
      "current step :  46\n",
      "reward :  -0.33437343345962534\n",
      "episode :  0\n",
      "current step :  47\n",
      "reward :  -0.6475127201956646\n",
      "episode :  0\n",
      "current step :  48\n",
      "reward :  -0.6701101328858664\n",
      "episode :  0\n",
      "current step :  49\n",
      "reward :  -0.5544496467113714\n",
      "episode :  0\n",
      "current step :  50\n",
      "reward :  -0.41371671004537836\n",
      "episode :  0\n",
      "current step :  51\n",
      "reward :  -0.4788141658295141\n",
      "episode :  0\n",
      "current step :  52\n",
      "reward :  -0.41403978894179244\n",
      "episode :  0\n",
      "current step :  53\n",
      "reward :  -0.4790497958050925\n",
      "episode :  0\n",
      "current step :  54\n",
      "reward :  -0.4344297828879119\n",
      "episode :  0\n",
      "current step :  55\n",
      "reward :  -0.3369016689055309\n",
      "episode :  0\n",
      "current step :  56\n",
      "reward :  -0.5069707578104115\n",
      "episode :  0\n",
      "current step :  57\n",
      "reward :  -0.6093712187019584\n",
      "episode :  0\n",
      "current step :  58\n",
      "reward :  -0.6987382914801672\n",
      "episode :  0\n",
      "current step :  59\n",
      "reward :  -0.4138593071007775\n",
      "episode :  0\n",
      "current step :  60\n",
      "reward :  -0.4335210158195998\n",
      "episode :  0\n",
      "current step :  61\n",
      "reward :  -0.46150115280555964\n",
      "episode :  0\n",
      "current step :  62\n",
      "reward :  -0.5436887213756453\n",
      "episode :  0\n",
      "current step :  63\n",
      "reward :  -0.6572052646301273\n",
      "episode :  0\n",
      "current step :  64\n",
      "reward :  -0.6602126915704862\n",
      "episode :  0\n",
      "current step :  65\n",
      "reward :  -0.5051740858627594\n",
      "episode :  0\n",
      "current step :  66\n",
      "reward :  -0.49503246246524674\n",
      "episode :  0\n",
      "current step :  67\n",
      "reward :  -0.4268008858479506\n",
      "episode :  0\n",
      "current step :  68\n",
      "reward :  -0.7390221360495535\n",
      "episode :  0\n",
      "current step :  69\n",
      "reward :  -0.6932202310038795\n",
      "episode :  0\n",
      "current step :  70\n",
      "reward :  -0.6214693169008304\n",
      "episode :  0\n",
      "current step :  71\n",
      "reward :  -0.6784578468387095\n",
      "episode :  0\n",
      "current step :  72\n",
      "reward :  -0.6957265962051167\n",
      "episode :  0\n",
      "current step :  73\n",
      "reward :  -0.7696142379778563\n",
      "episode :  0\n",
      "current step :  74\n",
      "reward :  -0.6707031337002465\n",
      "episode :  0\n",
      "current step :  75\n",
      "reward :  -0.6250273947900735\n",
      "episode :  0\n",
      "current step :  76\n",
      "reward :  -0.43410492437024384\n",
      "episode :  0\n",
      "current step :  77\n",
      "reward :  -0.415744716615306\n",
      "episode :  0\n",
      "current step :  78\n",
      "reward :  -0.46371416916318703\n",
      "episode :  0\n",
      "current step :  79\n",
      "reward :  -0.6343376735083103\n",
      "episode :  0\n",
      "current step :  80\n",
      "reward :  -0.6847472110629013\n",
      "episode :  0\n",
      "current step :  81\n",
      "reward :  -0.7764542732375843\n",
      "episode :  0\n",
      "current step :  82\n",
      "reward :  -0.589843548922831\n",
      "episode :  0\n",
      "current step :  83\n",
      "reward :  -0.6083558630597029\n",
      "episode :  0\n",
      "current step :  84\n",
      "reward :  -0.6858844889547762\n",
      "episode :  0\n",
      "current step :  85\n",
      "reward :  -0.4634695156913708\n",
      "episode :  0\n",
      "current step :  86\n",
      "reward :  -0.6765529622364932\n",
      "episode :  0\n",
      "current step :  87\n",
      "reward :  -0.7195392682466323\n",
      "episode :  0\n",
      "current step :  88\n",
      "reward :  -0.43650438161022453\n",
      "episode :  0\n",
      "current step :  89\n",
      "reward :  -0.42073213034156876\n",
      "episode :  0\n",
      "current step :  90\n",
      "reward :  -0.5128730779570329\n",
      "episode :  0\n",
      "current step :  91\n",
      "reward :  -0.4577383764763279\n",
      "episode :  0\n",
      "current step :  92\n",
      "reward :  -0.44494903855022605\n",
      "episode :  0\n",
      "current step :  93\n",
      "reward :  -0.5459834928366907\n",
      "episode :  0\n",
      "current step :  94\n",
      "reward :  -0.48511409048623205\n",
      "episode :  0\n",
      "current step :  95\n",
      "reward :  -0.36959317137927933\n",
      "episode :  0\n",
      "current step :  96\n",
      "reward :  -0.5177464733884037\n",
      "episode :  0\n",
      "current step :  97\n",
      "reward :  -0.42378119830278554\n",
      "episode :  0\n",
      "current step :  98\n",
      "reward :  -0.34358748521168936\n",
      "episode :  0\n",
      "current step :  99\n",
      "reward :  -0.31726435836558536\n",
      "episode :  0\n",
      "current step :  100\n",
      "reward :  -0.36649535759158625\n",
      "episode :  0\n",
      "current step :  101\n",
      "reward :  -0.3789860505019012\n",
      "episode :  0\n",
      "current step :  102\n",
      "reward :  -0.40382516071054964\n",
      "episode :  0\n",
      "current step :  103\n",
      "reward :  -0.43947005162565395\n",
      "episode :  0\n",
      "current step :  104\n",
      "reward :  -0.36795285928150073\n",
      "episode :  0\n",
      "current step :  105\n",
      "reward :  -0.2661361834133284\n",
      "episode :  0\n",
      "current step :  106\n",
      "reward :  -0.3673693805326246\n",
      "episode :  0\n",
      "current step :  107\n",
      "reward :  -0.3266258392485765\n",
      "episode :  0\n",
      "current step :  108\n",
      "reward :  -0.43823833595893735\n",
      "episode :  0\n",
      "current step :  109\n",
      "reward :  -0.41033894321611564\n",
      "episode :  0\n",
      "current step :  110\n",
      "reward :  -0.43875545178097053\n",
      "episode :  0\n",
      "current step :  111\n",
      "reward :  -0.41542519061517064\n",
      "episode :  0\n",
      "current step :  112\n",
      "reward :  -0.4329581918569275\n",
      "episode :  0\n",
      "current step :  113\n",
      "reward :  -0.4054270809239433\n",
      "episode :  0\n",
      "current step :  114\n",
      "reward :  -0.48945133361107485\n",
      "episode :  0\n",
      "current step :  115\n",
      "reward :  -0.325000535647352\n",
      "episode :  0\n",
      "current step :  116\n",
      "reward :  -0.5193097298059981\n",
      "episode :  0\n",
      "current step :  117\n",
      "reward :  -0.46409986630986355\n",
      "episode :  0\n",
      "current step :  118\n",
      "reward :  -0.4295108291695328\n",
      "episode :  0\n",
      "current step :  119\n",
      "reward :  -0.36791424941825895\n",
      "episode :  0\n",
      "current step :  120\n",
      "reward :  -0.42443701620222013\n",
      "episode :  0\n",
      "current step :  121\n",
      "reward :  -0.4668094303973973\n",
      "episode :  0\n",
      "current step :  122\n",
      "reward :  -0.33908328174931524\n",
      "episode :  0\n",
      "current step :  123\n",
      "reward :  -0.4538208366355008\n",
      "episode :  0\n",
      "current step :  124\n",
      "reward :  -0.4125103264272023\n",
      "episode :  0\n",
      "current step :  125\n",
      "reward :  -0.449523974591995\n",
      "episode :  0\n",
      "current step :  126\n",
      "reward :  -0.50318460394125\n",
      "episode :  0\n",
      "current step :  127\n",
      "reward :  -0.3655780655940098\n",
      "episode :  0\n",
      "current step :  128\n",
      "reward :  -0.41731980118656553\n",
      "episode :  0\n",
      "current step :  129\n",
      "reward :  -0.5073708172928392\n",
      "episode :  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step :  130\n",
      "reward :  -0.4574035210295705\n",
      "episode :  0\n",
      "current step :  131\n",
      "reward :  -0.4984373840656076\n",
      "episode :  0\n",
      "current step :  132\n",
      "reward :  -0.39185757012182254\n",
      "episode :  0\n",
      "current step :  133\n",
      "reward :  -0.4208950922854859\n",
      "episode :  0\n",
      "current step :  134\n",
      "reward :  -0.4055671184230996\n",
      "episode :  0\n",
      "current step :  135\n",
      "reward :  -0.32294673321945777\n",
      "episode :  0\n",
      "current step :  136\n",
      "reward :  -0.32120024757989674\n",
      "episode :  0\n",
      "current step :  137\n",
      "reward :  -0.45519060815320334\n",
      "episode :  0\n",
      "current step :  138\n",
      "reward :  -0.35894377177120185\n",
      "episode :  0\n",
      "current step :  139\n",
      "reward :  -0.46268888320985097\n",
      "episode :  0\n",
      "current step :  140\n",
      "reward :  -0.3674995380374069\n",
      "episode :  0\n",
      "current step :  141\n",
      "reward :  -0.4980969724622929\n",
      "episode :  0\n",
      "current step :  142\n",
      "reward :  -0.4259862988520666\n",
      "episode :  0\n",
      "current step :  143\n",
      "reward :  -0.37736954642635884\n",
      "episode :  0\n",
      "current step :  144\n",
      "reward :  -0.4198813874316939\n",
      "episode :  0\n",
      "current step :  145\n",
      "reward :  -0.4750584138512848\n",
      "episode :  0\n",
      "current step :  146\n",
      "reward :  -0.3902747572224778\n",
      "episode :  0\n",
      "current step :  147\n",
      "reward :  -0.49769247824907187\n",
      "episode :  0\n",
      "current step :  148\n",
      "reward :  -0.33994034208234436\n",
      "episode :  0\n",
      "current step :  149\n",
      "reward :  -0.4903726383449\n",
      "episode :  0\n",
      "current step :  150\n",
      "reward :  -0.43993794631321104\n",
      "episode :  0\n",
      "current step :  151\n",
      "reward :  -0.5024374530533784\n",
      "episode :  0\n",
      "current step :  152\n",
      "reward :  -0.5016803589108114\n",
      "episode :  0\n",
      "current step :  153\n",
      "reward :  -0.40116244536538787\n",
      "episode :  0\n",
      "current step :  154\n",
      "reward :  -0.43701087375065806\n",
      "episode :  0\n",
      "current step :  155\n",
      "reward :  -0.5000958820517427\n",
      "episode :  0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "model.learn(total_timesteps= 3, log_interval=10)\n",
    "model.save(\"ddpg_imitation\")\n",
    "vec_env = model.get_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265cdc2",
   "metadata": {},
   "source": [
    "For training, we can stop here. We can begin the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DDPG.load(\"ddpg_imitation\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812a3d7",
   "metadata": {},
   "source": [
    "The below is just some other tials, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba77d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "\n",
    "model = DDPG.load(\"ddpg_imitation\")\n",
    "model.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3088d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for t in env.targets:\n",
    "    action, _states = model.predict(np.array(t.flatten()).reshape(1,-1))\n",
    "    obs, rewards, dones, info = env.step(action.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314bfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
